{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GaXguL3XyCO",
        "outputId": "89f1ac5d-4678-48f8-a7b4-20848d7db587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKO8gkGsZk_h",
        "outputId": "abba2dd8-ab4d-4c35-fbd8-4a7958477460"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcJkP_DRaUqa",
        "outputId": "f5a347fe-b519-4671-83d3-7e15133b084f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Collecting openai\n",
            "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "Successfully installed openai-1.54.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7cpqSm5bhen",
        "outputId": "98bda236-619f-43b7-deb7-57856e46522d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, RepeatVector, TimeDistributed, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "0_2fRUBvcTIp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('embedded_gptneo.csv')\n",
        "data['embedding'] = data['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedding'].tolist())\n",
        "\n",
        "# Define columns and class counts\n",
        "target_columns = {\n",
        "    'provokingviolence': 4,\n",
        "    'individualharrassment': 4,\n",
        "    'emotionaldistress': 3\n",
        "}\n",
        "\n",
        "# One-hot encode each target column and prepare the dataset\n",
        "encoded_targets = {}\n",
        "for col, num_classes in target_columns.items():\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    encoded_targets[col] = encoder.fit_transform(data[col].values.reshape(-1, 1))\n",
        "\n",
        "# Concatenate one-hot encoded targets into a single array for easy indexing\n",
        "y = np.hstack([encoded_targets[col] for col in target_columns])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape X to add a time dimension for LSTM (seq_len=1 for static embeddings)\n",
        "X_train = X_train[:, np.newaxis, :]\n",
        "X_val = X_val[:, np.newaxis, :]\n",
        "\n",
        "# Create a custom Dataset class\n",
        "class MultiOutputDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input': torch.tensor(self.features[idx], dtype=torch.float32),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = MultiOutputDataset(X_train, y_train)\n",
        "val_dataset = MultiOutputDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the LSTM model with multiple output heads\n",
        "class MultiOutputLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dims, num_layers=1):\n",
        "        super(MultiOutputLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "        # Separate output layers for each target\n",
        "        self.output_heads = nn.ModuleDict({\n",
        "            target: nn.Linear(hidden_dim, output_dim) for target, output_dim in output_dims.items()\n",
        "        })\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        x = self.dropout(hn[-1])\n",
        "\n",
        "        # Return output from each output head\n",
        "        outputs = {target: head(x) for target, head in self.output_heads.items()}\n",
        "        return outputs\n",
        "\n",
        "# Instantiate the model with separate output layers for each target\n",
        "input_dim = X.shape[1]  # Number of features per timestep\n",
        "hidden_dim = 64         # Number of features in LSTM hidden state\n",
        "output_dims = {target: num_classes for target, num_classes in target_columns.items()}\n",
        "model = MultiOutputLSTM(input_dim, hidden_dim, output_dims)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-class with logits\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training function for multi-output model\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_data = batch['input'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_data)\n",
        "            # Compute loss separately for each output head\n",
        "            losses = [criterion(outputs[target], labels[:, start:end])\n",
        "                      for target, (start, end) in zip(target_columns.keys(),\n",
        "                                                      [(0,4), (4,8), (8,11)])]\n",
        "            loss = sum(losses)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = {}, {}\n",
        "\n",
        "    for target in target_columns.keys():\n",
        "        predictions[target], true_labels[target] = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_data = batch['input'].to(device)\n",
        "            labels = batch['label'].cpu().numpy()\n",
        "\n",
        "            outputs = model(input_data)\n",
        "            for target, pred in outputs.items():\n",
        "                predictions[target].append(pred.cpu().numpy())\n",
        "                start, end = (0,4) if target == 'provokingviolence' else ((4,8) if target == 'individualharrassment' else (8,11))\n",
        "                true_labels[target].append(labels[:, start:end])\n",
        "\n",
        "    # Concatenate batches\n",
        "    predictions = {target: np.vstack(preds) for target, preds in predictions.items()}\n",
        "    true_labels = {target: np.vstack(labels) for target, labels in true_labels.items()}\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, true_labels = evaluate_model(model, val_loader)\n",
        "\n",
        "# Apply sigmoid, threshold, and calculate metrics for each output head\n",
        "for target, num_classes in target_columns.items():\n",
        "    y_pred_binary = (torch.sigmoid(torch.tensor(predictions[target])) > 0.5).int().numpy()\n",
        "    y_true_binary = true_labels[target]\n",
        "\n",
        "    y_pred_labels = np.argmax(y_pred_binary, axis=1)\n",
        "    y_true_labels = np.argmax(y_true_binary, axis=1)\n",
        "\n",
        "    print(f\"Classification report for {target}:\")\n",
        "    print(classification_report(y_true_labels, y_pred_labels))\n",
        "\n",
        "    overall_accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
        "    print(f\"Overall Accuracy for {target}: {overall_accuracy:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiIZyaQ8cWs9",
        "outputId": "126d13d5-b324-43d4-820c-2f2a0fe19ac5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.2692\n",
            "Epoch 2/10, Loss: 1.2052\n",
            "Epoch 3/10, Loss: 1.1952\n",
            "Epoch 4/10, Loss: 1.1869\n",
            "Epoch 5/10, Loss: 1.1797\n",
            "Epoch 6/10, Loss: 1.1771\n",
            "Epoch 7/10, Loss: 1.1727\n",
            "Epoch 8/10, Loss: 1.1722\n",
            "Epoch 9/10, Loss: 1.1678\n",
            "Epoch 10/10, Loss: 1.1644\n",
            "Classification report for provokingviolence:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.36      0.36      1975\n",
            "           1       0.00      0.00      0.00       966\n",
            "           2       0.63      0.80      0.71      5855\n",
            "           3       0.79      0.61      0.69      2191\n",
            "\n",
            "    accuracy                           0.61     10987\n",
            "   macro avg       0.45      0.44      0.44     10987\n",
            "weighted avg       0.56      0.61      0.58     10987\n",
            "\n",
            "Overall Accuracy for provokingviolence: 0.6098\n",
            "\n",
            "Classification report for individualharrassment:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.01      0.48      0.02        81\n",
            "           1       0.57      0.16      0.25      2386\n",
            "           2       0.55      0.53      0.54      5430\n",
            "           3       0.57      0.27      0.36      3090\n",
            "\n",
            "    accuracy                           0.38     10987\n",
            "   macro avg       0.43      0.36      0.29     10987\n",
            "weighted avg       0.56      0.38      0.42     10987\n",
            "\n",
            "Overall Accuracy for individualharrassment: 0.3760\n",
            "\n",
            "Classification report for emotionaldistress:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.01      0.02      0.01       100\n",
            "           1       0.61      0.37      0.46      3151\n",
            "           2       0.79      0.90      0.84      7736\n",
            "\n",
            "    accuracy                           0.74     10987\n",
            "   macro avg       0.47      0.43      0.44     10987\n",
            "weighted avg       0.73      0.74      0.72     10987\n",
            "\n",
            "Overall Accuracy for emotionaldistress: 0.7387\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('embedded_gptneo.csv')\n",
        "\n",
        "# Convert the 'embedded_text' column to numpy arrays\n",
        "data['embedding'] = data['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "\n",
        "X = np.array(data['embedding'].tolist())\n",
        "\n",
        "# Define target columns and their respective number of classes\n",
        "target_columns = {\n",
        "    'provokingviolence': 4,\n",
        "    'individualharrassment': 4,\n",
        "    'emotionaldistress': 3\n",
        "}\n",
        "\n",
        "# One-hot encode each target column\n",
        "encoded_targets = {}\n",
        "encoders = {}\n",
        "for col, num_classes in target_columns.items():\n",
        "    encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "    encoded = encoder.fit_transform(data[col].values.reshape(-1, 1))\n",
        "    encoded_targets[col] = encoded\n",
        "    encoders[col] = encoder  # Save encoder for inverse transformations if needed\n",
        "\n",
        "# Concatenate one-hot encoded targets into a single array\n",
        "y = np.hstack([encoded_targets[col] for col in target_columns])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape X to add a time dimension for hierarchical LSTM (word-level)\n",
        "X_train = X_train[:, np.newaxis, :]\n",
        "X_val = X_val[:, np.newaxis, :]\n",
        "\n",
        "# Create a custom Dataset class\n",
        "class MultiOutputDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input': torch.tensor(self.features[idx], dtype=torch.float32),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = MultiOutputDataset(X_train, y_train)\n",
        "val_dataset = MultiOutputDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the Hierarchical BiLSTM model with multiple output heads\n",
        "class HierarchicalBiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dims, word_num_layers=1, sent_num_layers=1):\n",
        "        super(HierarchicalBiLSTM, self).__init__()\n",
        "\n",
        "        self.word_bilstm = nn.LSTM(\n",
        "            input_dim, hidden_dim, word_num_layers,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Sentence-level BiLSTM to capture sentence-level context\n",
        "        self.sent_bilstm = nn.LSTM(\n",
        "            hidden_dim * 2, hidden_dim, sent_num_layers,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Define output heads for each target\n",
        "        self.output_heads = nn.ModuleDict({\n",
        "            target: nn.Linear(hidden_dim * 2, output_dim)\n",
        "            for target, output_dim in output_dims.items()\n",
        "        })\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, num_words, word_embedding_dim)\n",
        "        batch_size, num_words, _ = x.size()\n",
        "\n",
        "        # Word-level BiLSTM\n",
        "        word_level_outputs, _ = self.word_bilstm(x)  # (batch_size, num_words, hidden_dim * 2)\n",
        "\n",
        "        # Get the sentence representation by aggregating word-level outputs\n",
        "        sentence_embedding = word_level_outputs.mean(dim=1)  # (batch_size, hidden_dim * 2)\n",
        "\n",
        "        # Sentence-level BiLSTM\n",
        "        sentence_embedding = sentence_embedding.unsqueeze(1)  # Add a pseudo-sequence length of 1\n",
        "        sentence_output, (hn, cn) = self.sent_bilstm(sentence_embedding)\n",
        "\n",
        "        # Aggregate forward and backward hidden states\n",
        "        sentence_output = sentence_output.squeeze(1)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(sentence_output)\n",
        "\n",
        "        # Compute outputs for each target\n",
        "        outputs = {target: head(x) for target, head in self.output_heads.items()}\n",
        "        return outputs\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = X.shape[1]  # Number of features per word embedding\n",
        "hidden_dim = 64         # Number of features in LSTM hidden state\n",
        "output_dims = {target: num_classes for target, num_classes in target_columns.items()}\n",
        "model = HierarchicalBiLSTM(input_dim, hidden_dim, output_dims)\n",
        "\n",
        "# Define loss functions for each output\n",
        "criteria = {\n",
        "    target: nn.CrossEntropyLoss()  # Adjust class weights if necessary\n",
        "    for target in target_columns\n",
        "}\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criteria, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_data = batch['input'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_data)\n",
        "\n",
        "            # Compute loss for each target\n",
        "            loss = 0\n",
        "            for idx, target in enumerate(target_columns.keys()):\n",
        "                start = sum(list(target_columns.values())[:idx])\n",
        "                end = start + target_columns[target]\n",
        "                target_labels = torch.argmax(labels[:, start:end], dim=1)\n",
        "                loss += criteria[target](outputs[target], target_labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criteria, optimizer, epochs=20)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    predictions = {target: [] for target in target_columns}\n",
        "    true_labels = {target: [] for target in target_columns}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_data = batch['input'].to(device)\n",
        "            labels = batch['label'].cpu().numpy()\n",
        "\n",
        "            outputs = model(input_data)\n",
        "            for idx, target in enumerate(target_columns.keys()):\n",
        "                preds = outputs[target].cpu().numpy()\n",
        "                predictions[target].append(preds)\n",
        "\n",
        "                start = sum(list(target_columns.values())[:idx])\n",
        "                end = start + target_columns[target]\n",
        "                true_labels[target].append(labels[:, start:end])\n",
        "\n",
        "    # Concatenate all batches\n",
        "    for target in target_columns:\n",
        "        predictions[target] = np.vstack(predictions[target])\n",
        "        true_labels[target] = np.vstack(true_labels[target])\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, true_labels = evaluate_model(model, val_loader)\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(predictions, true_labels, encoders, target_columns):\n",
        "    for target, num_classes in target_columns.items():\n",
        "        y_pred = np.argmax(predictions[target], axis=1)\n",
        "        y_true = np.argmax(true_labels[target], axis=1)\n",
        "\n",
        "        encoder = encoders[target]\n",
        "        target_names = [str(cls) for cls in encoder.categories_[0]]\n",
        "\n",
        "        print(f\"Classification Report for '{target}':\")\n",
        "        print(classification_report(y_true, y_pred, target_names=target_names))\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        print(f\"Overall Accuracy for '{target}': {accuracy:.4f}\\n\")\n",
        "\n",
        "# Display the metrics\n",
        "compute_metrics(predictions, true_labels, encoders, target_columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mncIENGyc1a4",
        "outputId": "c9276b7b-1dbf-436c-c9b0-9faeb1d4ac91"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.5110\n",
            "Epoch 2/20, Loss: 2.4145\n",
            "Epoch 3/20, Loss: 2.3910\n",
            "Epoch 4/20, Loss: 2.3732\n",
            "Epoch 5/20, Loss: 2.3599\n",
            "Epoch 6/20, Loss: 2.3468\n",
            "Epoch 7/20, Loss: 2.3372\n",
            "Epoch 8/20, Loss: 2.3259\n",
            "Epoch 9/20, Loss: 2.3149\n",
            "Epoch 10/20, Loss: 2.3086\n",
            "Epoch 11/20, Loss: 2.2979\n",
            "Epoch 12/20, Loss: 2.2866\n",
            "Epoch 13/20, Loss: 2.2773\n",
            "Epoch 14/20, Loss: 2.2677\n",
            "Epoch 15/20, Loss: 2.2604\n",
            "Epoch 16/20, Loss: 2.2506\n",
            "Epoch 17/20, Loss: 2.2433\n",
            "Epoch 18/20, Loss: 2.2401\n",
            "Epoch 19/20, Loss: 2.2315\n",
            "Epoch 20/20, Loss: 2.2255\n",
            "Classification Report for 'provokingviolence':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.23      0.32      1975\n",
            "           1       0.00      0.00      0.00       966\n",
            "           2       0.62      0.86      0.72      5855\n",
            "           3       0.75      0.67      0.71      2191\n",
            "\n",
            "    accuracy                           0.63     10987\n",
            "   macro avg       0.46      0.44      0.44     10987\n",
            "weighted avg       0.57      0.63      0.58     10987\n",
            "\n",
            "Overall Accuracy for 'provokingviolence': 0.6314\n",
            "\n",
            "Classification Report for 'individualharrassment':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        81\n",
            "           1       0.51      0.30      0.38      2386\n",
            "           2       0.53      0.77      0.63      5430\n",
            "           3       0.57      0.31      0.40      3090\n",
            "\n",
            "    accuracy                           0.54     10987\n",
            "   macro avg       0.40      0.35      0.35     10987\n",
            "weighted avg       0.53      0.54      0.51     10987\n",
            "\n",
            "Overall Accuracy for 'individualharrassment': 0.5359\n",
            "\n",
            "Classification Report for 'emotionaldistress':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       100\n",
            "           1       0.59      0.39      0.47      3151\n",
            "           2       0.78      0.90      0.83      7736\n",
            "\n",
            "    accuracy                           0.74     10987\n",
            "   macro avg       0.46      0.43      0.43     10987\n",
            "weighted avg       0.72      0.74      0.72     10987\n",
            "\n",
            "Overall Accuracy for 'emotionaldistress': 0.7427\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('embedded_gptneo.csv')\n",
        "data['embedding'] = data['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "\n",
        "X = np.array(data['embedding'].tolist())\n",
        "\n",
        "# Define target columns and their respective number of classes\n",
        "target_columns = {\n",
        "    'provokingviolence': 4,\n",
        "    'individualharrassment': 4,\n",
        "    'emotionaldistress': 3\n",
        "}\n",
        "\n",
        "# One-hot encode each target column\n",
        "encoded_targets = {}\n",
        "encoders = {}\n",
        "for col, num_classes in target_columns.items():\n",
        "    encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
        "    encoded = encoder.fit_transform(data[col].values.reshape(-1, 1))\n",
        "    encoded_targets[col] = encoded\n",
        "    encoders[col] = encoder\n",
        "\n",
        "# Concatenate one-hot encoded targets into a single array\n",
        "y = np.hstack([encoded_targets[col] for col in target_columns])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape X to add a time dimension for MTM LSTM\n",
        "X_train = X_train[:, np.newaxis, :]\n",
        "X_val = X_val[:, np.newaxis, :]\n",
        "\n",
        "# Create a custom Dataset class\n",
        "class MultiOutputDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input': torch.tensor(self.features[idx], dtype=torch.float32),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = MultiOutputDataset(X_train, y_train)\n",
        "val_dataset = MultiOutputDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the MTM LSTM model\n",
        "class MTMLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dims, num_layers=1):\n",
        "        super(MTMLSTM, self).__init__()\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim, hidden_dim, num_layers,\n",
        "            batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Define output heads for each target\n",
        "        self.output_heads = nn.ModuleDict({\n",
        "            target: nn.Linear(hidden_dim * 2, output_dim)\n",
        "            for target, output_dim in output_dims.items()\n",
        "        })\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LSTM output\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Compute outputs for each target\n",
        "        outputs = {target: head(lstm_out[:, -1, :]) for target, head in self.output_heads.items()}\n",
        "        return outputs\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = X.shape[1]  # Number of features per word embedding\n",
        "hidden_dim = 64         # Number of features in LSTM hidden state\n",
        "output_dims = {target: num_classes for target, num_classes in target_columns.items()}\n",
        "model = MTMLSTM(input_dim, hidden_dim, output_dims)\n",
        "\n",
        "# Define loss functions for each output\n",
        "criteria = {\n",
        "    target: nn.CrossEntropyLoss()\n",
        "    for target in target_columns\n",
        "}\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criteria, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_data = batch['input'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_data)\n",
        "\n",
        "            # Compute loss for each target\n",
        "            loss = 0\n",
        "            for idx, target in enumerate(target_columns.keys()):\n",
        "                start = sum(list(target_columns.values())[:idx])\n",
        "                end = start + target_columns[target]\n",
        "                target_labels = torch.argmax(labels[:, start:end], dim=1)\n",
        "                loss += criteria[target](outputs[target], target_labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criteria, optimizer, epochs=10)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    predictions = {target: [] for target in target_columns}\n",
        "    true_labels = {target: [] for target in target_columns}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_data = batch['input'].to(device)\n",
        "            labels = batch['label'].cpu().numpy()\n",
        "\n",
        "            outputs = model(input_data)\n",
        "            for idx, target in enumerate(target_columns.keys()):\n",
        "                preds = outputs[target].cpu().numpy()\n",
        "                predictions[target].append(preds)\n",
        "\n",
        "                start = sum(list(target_columns.values())[:idx])\n",
        "                end = start + target_columns[target]\n",
        "                true_labels[target].append(labels[:, start:end])\n",
        "\n",
        "    for target in target_columns:\n",
        "        predictions[target] = np.vstack(predictions[target])\n",
        "        true_labels[target] = np.vstack(true_labels[target])\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, true_labels = evaluate_model(model, val_loader)\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(predictions, true_labels, encoders, target_columns):\n",
        "    for target, num_classes in target_columns.items():\n",
        "        y_pred = np.argmax(predictions[target], axis=1)\n",
        "        y_true = np.argmax(true_labels[target], axis=1)\n",
        "\n",
        "        encoder = encoders[target]\n",
        "        target_names = [str(cls) for cls in encoder.categories_[0]]\n",
        "\n",
        "        print(f\"Classification Report for '{target}':\")\n",
        "        print(classification_report(y_true, y_pred, target_names=target_names))\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        print(f\"Overall Accuracy for '{target}': {accuracy:.4f}\\n\")\n",
        "\n",
        "# Display the metrics\n",
        "compute_metrics(predictions, true_labels, encoders, target_columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4C3p-GMd3Qk",
        "outputId": "704b6adb-964c-4ec5-83d8-4e80111fc997"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 2.5366\n",
            "Epoch 2/10, Loss: 2.4455\n",
            "Epoch 3/10, Loss: 2.4235\n",
            "Epoch 4/10, Loss: 2.4088\n",
            "Epoch 5/10, Loss: 2.3971\n",
            "Epoch 6/10, Loss: 2.3916\n",
            "Epoch 7/10, Loss: 2.3766\n",
            "Epoch 8/10, Loss: 2.3785\n",
            "Epoch 9/10, Loss: 2.3702\n",
            "Epoch 10/10, Loss: 2.3657\n",
            "Classification Report for 'provokingviolence':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.29      0.36      1975\n",
            "           1       0.00      0.00      0.00       966\n",
            "           2       0.62      0.85      0.72      5855\n",
            "           3       0.77      0.64      0.70      2191\n",
            "\n",
            "    accuracy                           0.63     10987\n",
            "   macro avg       0.47      0.44      0.44     10987\n",
            "weighted avg       0.57      0.63      0.59     10987\n",
            "\n",
            "Overall Accuracy for 'provokingviolence': 0.6314\n",
            "\n",
            "Classification Report for 'individualharrassment':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        81\n",
            "           1       0.50      0.36      0.42      2386\n",
            "           2       0.53      0.77      0.63      5430\n",
            "           3       0.56      0.26      0.35      3090\n",
            "\n",
            "    accuracy                           0.53     10987\n",
            "   macro avg       0.40      0.35      0.35     10987\n",
            "weighted avg       0.53      0.53      0.50     10987\n",
            "\n",
            "Overall Accuracy for 'individualharrassment': 0.5308\n",
            "\n",
            "Classification Report for 'emotionaldistress':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       100\n",
            "           1       0.58      0.44      0.50      3151\n",
            "           2       0.79      0.88      0.83      7736\n",
            "\n",
            "    accuracy                           0.75     10987\n",
            "   macro avg       0.46      0.44      0.45     10987\n",
            "weighted avg       0.72      0.75      0.73     10987\n",
            "\n",
            "Overall Accuracy for 'emotionaldistress': 0.7467\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('embedded_gptneo.csv')\n",
        "\n",
        "# Convert the 'embedding' column to numpy arrays\n",
        "data['embedding'] = data['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedding'].tolist())\n",
        "\n",
        "# Define target columns and their respective number of classes\n",
        "target_columns = {\n",
        "    'provokingviolence': 4,\n",
        "    'individualharrassment': 4,\n",
        "    'emotionaldistress': 3\n",
        "}\n",
        "\n",
        "# One-hot encode each target column\n",
        "encoded_targets = {}\n",
        "encoders = {}\n",
        "for col, num_classes in target_columns.items():\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    encoded = encoder.fit_transform(data[col].values.reshape(-1, 1))\n",
        "    encoded_targets[col] = encoded\n",
        "    encoders[col] = encoder\n",
        "\n",
        "# Concatenate one-hot encoded targets into a single array\n",
        "y = np.hstack([encoded_targets[col] for col in target_columns])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Dataset class\n",
        "class MultiOutputDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input': torch.tensor(self.features[idx], dtype=torch.float32),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = MultiOutputDataset(X_train, y_train)\n",
        "val_dataset = MultiOutputDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the Multi-Output MLP model\n",
        "class MultiOutputMLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dims):\n",
        "        super(MultiOutputMLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)          # Second hidden layer\n",
        "        self.dropout = nn.Dropout(0.3)         # Dropout for regularization\n",
        "\n",
        "        # Separate output heads for each target\n",
        "        self.output_heads = nn.ModuleDict({\n",
        "            target: nn.Linear(64, output_dim)\n",
        "            for target, output_dim in output_dims.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))            # Activation function\n",
        "        x = self.dropout(x)                     # Apply dropout\n",
        "        x = torch.relu(self.fc2(x))            # Activation function\n",
        "\n",
        "        # Compute outputs for each target\n",
        "        outputs = {target: head(x) for target, head in self.output_heads.items()}\n",
        "        return outputs\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = X.shape[1]  # Number of features\n",
        "output_dims = {col: num_classes for col, num_classes in target_columns.items()}\n",
        "model = MultiOutputMLPClassifier(input_dim, output_dims)\n",
        "\n",
        "# Define loss functions for each output\n",
        "criteria = {\n",
        "    target: nn.BCEWithLogitsLoss()\n",
        "    for target in target_columns\n",
        "}\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criteria, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_data = batch['input'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_data)\n",
        "\n",
        "            # Compute loss for each target\n",
        "            loss = 0\n",
        "            for idx, target in enumerate(target_columns.keys()):\n",
        "                start = sum(list(target_columns.values())[:idx])\n",
        "                end = start + target_columns[target]\n",
        "                target_labels = labels[:, start:end]\n",
        "                loss += criteria[target](outputs[target], target_labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criteria, optimizer, epochs=10)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    predictions = {target: [] for target in target_columns}\n",
        "    true_labels = {target: [] for target in target_columns}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_data = batch['input'].to(device)\n",
        "            labels = batch['label'].cpu().numpy()\n",
        "\n",
        "            outputs = model(input_data)\n",
        "            for idx, target in enumerate(target_columns.keys()):\n",
        "                preds = outputs[target].cpu().numpy()\n",
        "                predictions[target].append(preds)\n",
        "\n",
        "                start = sum(list(target_columns.values())[:idx])\n",
        "                end = start + target_columns[target]\n",
        "                true_labels[target].append(labels[:, start:end])\n",
        "\n",
        "    for target in target_columns:\n",
        "        predictions[target] = np.vstack(predictions[target])\n",
        "        true_labels[target] = np.vstack(true_labels[target])\n",
        "\n",
        "    return predictions, true_labels\n",
        "\n",
        "# Evaluate the model\n",
        "predictions, true_labels = evaluate_model(model, val_loader)\n",
        "\n",
        "# Compute metrics for each target\n",
        "def compute_metrics(predictions, true_labels, encoders, target_columns):\n",
        "    for target, num_classes in target_columns.items():\n",
        "        y_pred = (torch.sigmoid(torch.tensor(predictions[target])) > 0.5).int().numpy()\n",
        "        y_true = true_labels[target]\n",
        "\n",
        "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "        y_true_labels = np.argmax(y_true, axis=1)\n",
        "\n",
        "        encoder = encoders[target]\n",
        "        target_names = [str(cls) for cls in encoder.categories_[0]]\n",
        "\n",
        "        print(f\"Classification Report for '{target}':\")\n",
        "        print(classification_report(y_true_labels, y_pred_labels, target_names=target_names))\n",
        "\n",
        "        accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
        "        print(f\"Overall Accuracy for '{target}': {accuracy:.4f}\\n\")\n",
        "\n",
        "# Display the metrics\n",
        "compute_metrics(predictions, true_labels, encoders, target_columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz6Ubp_we7RE",
        "outputId": "829451d0-4caa-43e9-a995-06f50c2e2a68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.2254\n",
            "Epoch 2/10, Loss: 1.1850\n",
            "Epoch 3/10, Loss: 1.1746\n",
            "Epoch 4/10, Loss: 1.1678\n",
            "Epoch 5/10, Loss: 1.1631\n",
            "Epoch 6/10, Loss: 1.1583\n",
            "Epoch 7/10, Loss: 1.1551\n",
            "Epoch 8/10, Loss: 1.1525\n",
            "Epoch 9/10, Loss: 1.1505\n",
            "Epoch 10/10, Loss: 1.1478\n",
            "Classification Report for 'provokingviolence':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.34      0.38      1975\n",
            "           1       0.00      0.00      0.00       966\n",
            "           2       0.63      0.80      0.71      5855\n",
            "           3       0.75      0.68      0.72      2191\n",
            "\n",
            "    accuracy                           0.63     10987\n",
            "   macro avg       0.45      0.46      0.45     10987\n",
            "weighted avg       0.56      0.63      0.59     10987\n",
            "\n",
            "Overall Accuracy for 'provokingviolence': 0.6250\n",
            "\n",
            "Classification Report for 'individualharrassment':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.01      0.42      0.02        81\n",
            "           1       0.57      0.19      0.29      2386\n",
            "           2       0.56      0.50      0.53      5430\n",
            "           3       0.58      0.28      0.37      3090\n",
            "\n",
            "    accuracy                           0.37     10987\n",
            "   macro avg       0.43      0.35      0.30     10987\n",
            "weighted avg       0.56      0.37      0.43     10987\n",
            "\n",
            "Overall Accuracy for 'individualharrassment': 0.3700\n",
            "\n",
            "Classification Report for 'emotionaldistress':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.03      0.04      0.04       100\n",
            "           1       0.61      0.40      0.48      3151\n",
            "           2       0.79      0.90      0.84      7736\n",
            "\n",
            "    accuracy                           0.75     10987\n",
            "   macro avg       0.48      0.45      0.45     10987\n",
            "weighted avg       0.73      0.75      0.73     10987\n",
            "\n",
            "Overall Accuracy for 'emotionaldistress': 0.7461\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('embedded_gptneo.csv')\n",
        "\n",
        "# Convert the 'embedded_text' column to numpy arrays\n",
        "data['embedding'] = data['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedding'].tolist())\n",
        "\n",
        "# Define the target columns and initialize label encoders for each\n",
        "target_columns = ['provokingviolence', 'individualharrassment', 'emotionaldistress']\n",
        "label_encoders = {col: LabelEncoder() for col in target_columns}\n",
        "\n",
        "# Encode the labels for each target column\n",
        "y_encoded = {}\n",
        "for col in target_columns:\n",
        "    y_encoded[col] = label_encoders[col].fit_transform(data[col])\n",
        "\n",
        "# Split data into training and validation sets for each target column\n",
        "train_test_splits = {}\n",
        "for col in target_columns:\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y_encoded[col], test_size=0.2, random_state=42)\n",
        "    train_test_splits[col] = (X_train, X_val, y_train, y_val)\n",
        "\n",
        "# Function to train and evaluate XGBoost for each target\n",
        "def train_evaluate_xgboost(target_column):\n",
        "    X_train, X_val, y_train, y_val = train_test_splits[target_column]\n",
        "\n",
        "    # Initialize XGBoost classifier with suitable parameters\n",
        "    model = xgb.XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=len(label_encoders[target_column].classes_),  # Number of classes for the target\n",
        "        eval_metric='mlogloss',\n",
        "        use_label_encoder=False,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        n_estimators=100,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on validation data\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Convert predictions and true labels back to original labels\n",
        "    y_pred_labels = label_encoders[target_column].inverse_transform(y_pred)\n",
        "    y_val_labels = label_encoders[target_column].inverse_transform(y_val)\n",
        "\n",
        "    # Print classification report and accuracy\n",
        "    print(f\"Classification Report for '{target_column}':\")\n",
        "    print(classification_report(y_val_labels, y_pred_labels))\n",
        "    accuracy = accuracy_score(y_val_labels, y_pred_labels)\n",
        "    print(f\"Overall Accuracy for '{target_column}': {accuracy:.4f}\\n\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train and evaluate XGBoost model for each target column\n",
        "models = {}\n",
        "for col in target_columns:\n",
        "    print(f\"Training and evaluating model for target: {col}\")\n",
        "    models[col] = train_evaluate_xgboost(col)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OVRINrLfk3S",
        "outputId": "f5332091-db6e-46c7-8eb8-5ece8fec6362"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating model for target: provokingviolence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:41:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 'provokingviolence':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.22      0.31      1975\n",
            "           1       0.15      0.01      0.01       966\n",
            "           2       0.62      0.86      0.72      5855\n",
            "           3       0.75      0.69      0.72      2191\n",
            "\n",
            "    accuracy                           0.63     10987\n",
            "   macro avg       0.51      0.44      0.44     10987\n",
            "weighted avg       0.58      0.63      0.58     10987\n",
            "\n",
            "Overall Accuracy for 'provokingviolence': 0.6347\n",
            "\n",
            "Training and evaluating model for target: individualharrassment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:45:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 'individualharrassment':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.01      0.02        81\n",
            "           1       0.51      0.29      0.37      2386\n",
            "           2       0.53      0.77      0.63      5430\n",
            "           3       0.56      0.32      0.40      3090\n",
            "\n",
            "    accuracy                           0.53     10987\n",
            "   macro avg       0.45      0.35      0.36     10987\n",
            "weighted avg       0.53      0.53      0.51     10987\n",
            "\n",
            "Overall Accuracy for 'individualharrassment': 0.5342\n",
            "\n",
            "Training and evaluating model for target: emotionaldistress\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:48:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for 'emotionaldistress':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       100\n",
            "           1       0.59      0.40      0.48      3151\n",
            "           2       0.78      0.90      0.84      7736\n",
            "\n",
            "    accuracy                           0.75     10987\n",
            "   macro avg       0.46      0.43      0.44     10987\n",
            "weighted avg       0.72      0.75      0.73     10987\n",
            "\n",
            "Overall Accuracy for 'emotionaldistress': 0.7477\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqGExKOgDi3",
        "outputId": "f06c3aa6-41ce-41e2-fc16-f52c69287daa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load your dataset\n",
        "data_file = 'embedded_gptneo.csv'  # Adjust this to your dataset path\n",
        "data = pd.read_csv(data_file)\n",
        "\n",
        "# Assume your dataset has the text and the labels in the following columns\n",
        "texts = data['embedding'].tolist()  # Column with your input text\n",
        "labels = data[['provokingviolence', 'individualharrassment', 'emotionaldistress']]  # Adjust based on your actual column names\n",
        "\n",
        "# Tokenization parameters\n",
        "MAX_NB_WORDS = 50000\n",
        "MAX_SEQUENCE_LENGTH = 350\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Tokenization and padding\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "X = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Convert labels to categorical for each output\n",
        "Y_provoking = pd.get_dummies(labels['provokingviolence']).values\n",
        "Y_harassment = pd.get_dummies(labels['individualharrassment']).values\n",
        "Y_distress = pd.get_dummies(labels['emotionaldistress']).values\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, Y_train_provoking, Y_test_provoking = train_test_split(X, Y_provoking, test_size=0.30, random_state=1)\n",
        "_, _, Y_train_harassment, Y_test_harassment = train_test_split(X, Y_harassment, test_size=0.30, random_state=1)\n",
        "_, _, Y_train_distress, Y_test_distress = train_test_split(X, Y_distress, test_size=0.30, random_state=1)\n",
        "\n",
        "# Model architecture with multiple outputs\n",
        "input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(input_layer)\n",
        "x = SpatialDropout1D(0.2)(embedding_layer)\n",
        "x = Bidirectional(LSTM(200, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "\n",
        "# Define separate output layers for each label\n",
        "output_provoking = Dense(4, activation='softmax', name='provokingviolence')(x)\n",
        "output_harassment = Dense(4, activation='softmax', name='individualharrassment')(x)\n",
        "output_distress = Dense(3, activation='softmax', name='emotionaldistress')(x)\n",
        "\n",
        "# Model architecture with multiple outputs\n",
        "model = Model(inputs=input_layer, outputs=[output_provoking, output_harassment, output_distress])\n",
        "\n",
        "# Compile multi-output model with separate metrics for each output\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', 'accuracy', 'accuracy'])  # One metric for each output\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "epochs = 10  # Adjust the number of epochs as needed\n",
        "batch_size = 64\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    [Y_train_provoking, Y_train_harassment, Y_train_distress],\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001)]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_results = model.evaluate(X_test, [Y_test_provoking, Y_test_harassment, Y_test_distress])\n",
        "print(f\"Evaluation Results: {test_results}\")\n",
        "\n",
        "# Predict and evaluate each output independently\n",
        "preds_provoking, preds_harassment, preds_distress = model.predict(X_test)\n",
        "\n",
        "# Convert predictions to binary for each label\n",
        "preds_provoking_binary = (preds_provoking == preds_provoking.max(axis=1, keepdims=1)).astype(int)\n",
        "preds_harassment_binary = (preds_harassment == preds_harassment.max(axis=1, keepdims=1)).astype(int)\n",
        "preds_distress_binary = (preds_distress == preds_distress.max(axis=1, keepdims=1)).astype(int)\n",
        "\n",
        "# Evaluate classification metrics\n",
        "print(\"Classification Report for Provoking Violence:\")\n",
        "print(classification_report(Y_test_provoking.argmax(axis=1), preds_provoking_binary.argmax(axis=1)))\n",
        "\n",
        "print(\"Classification Report for Individual Harassment:\")\n",
        "print(classification_report(Y_test_harassment.argmax(axis=1), preds_harassment_binary.argmax(axis=1)))\n",
        "\n",
        "print(\"Classification Report for Emotional Distress:\")\n",
        "print(classification_report(Y_test_distress.argmax(axis=1), preds_distress_binary.argmax(axis=1)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HiGTcWX9gV66",
        "outputId": "3226f4ac-3e6a-4c98-fa82-aa1da3f81939"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m350\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m350\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │      \u001b[38;5;34m5,000,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ spatial_dropout1d         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m350\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │        \u001b[38;5;34m481,600\u001b[0m │ spatial_dropout1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ provokingviolence (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │          \u001b[38;5;34m1,604\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ individualharrassment     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │          \u001b[38;5;34m1,604\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ emotionaldistress (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │          \u001b[38;5;34m1,203\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">350</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">350</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,000,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ spatial_dropout1d         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">350</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)        │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bidirectional             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">481,600</span> │ spatial_dropout1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ provokingviolence (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,604</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ individualharrassment     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,604</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ emotionaldistress (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,203</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,486,011\u001b[0m (20.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,486,011</span> (20.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,486,011\u001b[0m (20.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,486,011</span> (20.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/10\n",
            "\u001b[1m541/541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 1s/step - emotionaldistress_accuracy: 0.6994 - individualharrassment_accuracy: 0.5119 - loss: 2.7548 - provokingviolence_accuracy: 0.5790 - val_emotionaldistress_accuracy: 0.7101 - val_individualharrassment_accuracy: 0.5257 - val_loss: 2.5268 - val_provokingviolence_accuracy: 0.6183\n",
            "Epoch 2/10\n",
            "\u001b[1m541/541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m744s\u001b[0m 1s/step - emotionaldistress_accuracy: 0.7108 - individualharrassment_accuracy: 0.5261 - loss: 2.5461 - provokingviolence_accuracy: 0.6111 - val_emotionaldistress_accuracy: 0.7028 - val_individualharrassment_accuracy: 0.5255 - val_loss: 2.5349 - val_provokingviolence_accuracy: 0.6157\n",
            "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 281ms/step - emotionaldistress_accuracy: 0.7004 - individualharrassment_accuracy: 0.5152 - loss: 2.5588 - provokingviolence_accuracy: 0.6129\n",
            "Evaluation Results: [2.5607388019561768, 0.7003033757209778, 0.5154733061790466, 0.6121966242790222]\n",
            "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 275ms/step\n",
            "Classification Report for Provoking Violence:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.02      0.04      3003\n",
            "           1       0.00      0.00      0.00      1429\n",
            "           2       0.58      0.95      0.72      8788\n",
            "           3       0.81      0.51      0.62      3260\n",
            "\n",
            "    accuracy                           0.61     16480\n",
            "   macro avg       0.48      0.37      0.35     16480\n",
            "weighted avg       0.57      0.61      0.52     16480\n",
            "\n",
            "Classification Report for Individual Harassment:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       125\n",
            "           1       0.53      0.05      0.09      3638\n",
            "           2       0.51      0.93      0.66      8117\n",
            "           3       0.63      0.16      0.26      4600\n",
            "\n",
            "    accuracy                           0.52     16480\n",
            "   macro avg       0.42      0.29      0.25     16480\n",
            "weighted avg       0.54      0.52      0.42     16480\n",
            "\n",
            "Classification Report for Emotional Distress:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       144\n",
            "           1       0.48      0.23      0.31      4736\n",
            "           2       0.74      0.90      0.81     11600\n",
            "\n",
            "    accuracy                           0.70     16480\n",
            "   macro avg       0.40      0.38      0.37     16480\n",
            "weighted avg       0.65      0.70      0.66     16480\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('embedded_gptneo.csv')  # Your dataset with precomputed embeddings\n",
        "\n",
        "# Assuming 'embedded_text' contains lists of embeddings as strings\n",
        "# Convert the string representations of lists to actual lists\n",
        "X = np.array(data['embedding'].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=',')).tolist())\n",
        "\n",
        "# Check the shape of X after conversion\n",
        "print(f\"Shape of X after converting: {X.shape}\")\n",
        "\n",
        "# Prepare target variables as one-hot encoded arrays\n",
        "Y_provoking = pd.get_dummies(labels['provokingviolence']).values\n",
        "Y_harassment = pd.get_dummies(labels['individualharrassment']).values\n",
        "Y_distress = pd.get_dummies(labels['emotionaldistress']).values\n",
        "\n",
        "# Check shapes of Y as well\n",
        "print(f\"Shapes of Y: Provoking: {Y_provoking.shape}, Harassment: {Y_harassment.shape}, Distress: {Y_distress.shape}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, Y_train_provoking, Y_test_provoking = train_test_split(X, Y_provoking, test_size=0.3, random_state=42)\n",
        "_, _, Y_train_harassment, Y_test_harassment = train_test_split(X, Y_harassment, test_size=0.3, random_state=42)\n",
        "_, _, Y_train_distress, Y_test_distress = train_test_split(X, Y_distress, test_size=0.3, random_state=42)\n",
        "\n",
        "# Build the multi-task LSTM model\n",
        "input_layer = Input(shape=(X.shape[1],))  # Adjust input shape based on your embeddings\n",
        "x = Dense(256, activation='relu')(input_layer)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "\n",
        "# Output layers for each task\n",
        "output_provoking = Dense(4, activation='softmax', name='provokingviolence')(x)\n",
        "output_harassment = Dense(4, activation='softmax', name='individualharrassment')(x)\n",
        "output_distress = Dense(3, activation='softmax', name='emotionaldistress')(x)\n",
        "\n",
        "metrics = ['accuracy'] * 3\n",
        "# Compile the model\n",
        "model = Model(inputs=input_layer, outputs=[output_provoking, output_harassment, output_distress])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = metrics)\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    [Y_train_provoking, Y_train_harassment, Y_train_distress],\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001)]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_results = model.evaluate(X_test, [Y_test_provoking, Y_test_harassment, Y_test_distress])\n",
        "print(f\"Evaluation Results: {test_results}\")\n",
        "\n",
        "# Predict and evaluate\n",
        "preds_provoking, preds_harassment, preds_distress = model.predict(X_test)\n",
        "\n",
        "# Classification reports\n",
        "print(\"Classification Report for Provoking Violence:\")\n",
        "print(classification_report(Y_test_provoking.argmax(axis=1), preds_provoking.argmax(axis=1)))\n",
        "\n",
        "print(\"Classification Report for Individual Harassment:\")\n",
        "print(classification_report(Y_test_harassment.argmax(axis=1), preds_harassment.argmax(axis=1)))\n",
        "\n",
        "print(\"Classification Report for Emotional Distress:\")\n",
        "print(classification_report(Y_test_distress.argmax(axis=1), preds_distress.argmax(axis=1)))\n",
        "\n",
        "# Define the squared Euclidean distance function\n",
        "def squared_euclidean_distance(y_true, y_pred):\n",
        "    return K.sum(K.square(y_true - y_pred), axis=-1)\n",
        "\n",
        "# Example usage of squared Euclidean distance\n",
        "# This should be part of a custom metric if needed\n",
        "# distance = squared_euclidean_distance(Y_test_provoking, preds_provoking)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1QJXkSsgW-1",
        "outputId": "494d3bbc-dc90-4902-ec25-0e82c4473ddb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after converting: (54932, 768)\n",
            "Shapes of Y: Provoking: (54932, 4), Harassment: (54932, 4), Distress: (54932, 3)\n",
            "Epoch 1/10\n",
            "\u001b[1m541/541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - emotionaldistress_accuracy: 0.6845 - individualharrassment_accuracy: 0.4598 - loss: 3.1622 - provokingviolence_accuracy: 0.5359 - val_emotionaldistress_accuracy: 0.7327 - val_individualharrassment_accuracy: 0.5346 - val_loss: 2.4747 - val_provokingviolence_accuracy: 0.6113\n",
            "Epoch 2/10\n",
            "\u001b[1m541/541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - emotionaldistress_accuracy: 0.7275 - individualharrassment_accuracy: 0.5185 - loss: 2.5293 - provokingviolence_accuracy: 0.6054 - val_emotionaldistress_accuracy: 0.7379 - val_individualharrassment_accuracy: 0.5473 - val_loss: 2.4286 - val_provokingviolence_accuracy: 0.6188\n",
            "Epoch 3/10\n",
            "\u001b[1m541/541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - emotionaldistress_accuracy: 0.7294 - individualharrassment_accuracy: 0.5191 - loss: 2.4913 - provokingviolence_accuracy: 0.6122 - val_emotionaldistress_accuracy: 0.7319 - val_individualharrassment_accuracy: 0.5432 - val_loss: 2.4078 - val_provokingviolence_accuracy: 0.6217\n",
            "Epoch 4/10\n",
            "\u001b[1m541/541\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - emotionaldistress_accuracy: 0.7427 - individualharrassment_accuracy: 0.5288 - loss: 2.4554 - provokingviolence_accuracy: 0.6176 - val_emotionaldistress_accuracy: 0.7353 - val_individualharrassment_accuracy: 0.5367 - val_loss: 2.4245 - val_provokingviolence_accuracy: 0.6209\n",
            "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - emotionaldistress_accuracy: 0.7349 - individualharrassment_accuracy: 0.5252 - loss: 2.4427 - provokingviolence_accuracy: 0.6279\n",
            "Evaluation Results: [2.4433629512786865, 0.7341019511222839, 0.5239077806472778, 0.6248786449432373]\n",
            "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Classification Report for Provoking Violence:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.07      0.12      2937\n",
            "           1       0.00      0.00      0.00      1439\n",
            "           2       0.60      0.90      0.72      8790\n",
            "           3       0.74      0.67      0.70      3314\n",
            "\n",
            "    accuracy                           0.62     16480\n",
            "   macro avg       0.46      0.41      0.39     16480\n",
            "weighted avg       0.56      0.62      0.55     16480\n",
            "\n",
            "Classification Report for Individual Harassment:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       125\n",
            "           1       0.59      0.10      0.18      3589\n",
            "           2       0.51      0.93      0.66      8172\n",
            "           3       0.63      0.14      0.23      4594\n",
            "\n",
            "    accuracy                           0.52     16480\n",
            "   macro avg       0.43      0.29      0.27     16480\n",
            "weighted avg       0.56      0.52      0.43     16480\n",
            "\n",
            "Classification Report for Emotional Distress:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       140\n",
            "           1       0.64      0.21      0.32      4751\n",
            "           2       0.74      0.96      0.84     11589\n",
            "\n",
            "    accuracy                           0.73     16480\n",
            "   macro avg       0.46      0.39      0.39     16480\n",
            "weighted avg       0.71      0.73      0.68     16480\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####\n",
        "###\n",
        "# Hybrid CNN attention _ Multilabel\n",
        "###\n",
        "#####\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('embedded_gptneo.csv')\n",
        "\n",
        "# Convert 'embedding' to numpy arrays\n",
        "data['embedding'] = data['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedding'].tolist())\n",
        "y = data[['emotionaldistress', 'provokingviolence', 'individualharrassment']].values\n",
        "\n",
        "# Convert y to binary format (multi-label)\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dataset class\n",
        "class MultilabelDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.X[idx], 'labels': self.y[idx]}\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = MultilabelDataset(X_train, y_train)\n",
        "val_dataset = MultilabelDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Hybrid CNN-Attention model\n",
        "class HybridMultilabelClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(HybridMultilabelClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        # Fully connected layer after flattening\n",
        "        self.fc1 = nn.Linear(128 * (input_dim // 2), 64)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(64, 1)\n",
        "\n",
        "        # Final fully connected layer for classification\n",
        "        self.fc2 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add channel dimension for Conv1d, Shape: [batch_size, 1, input_dim]\n",
        "        x = torch.relu(self.conv1(x))  # Shape: [batch_size, 128, input_dim]\n",
        "        x = self.pool1(x)  # Shape: [batch_size, 128, input_dim//2]\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten for fully connected layer\n",
        "        x = torch.relu(self.fc1(x))  # Shape: [batch_size, 64]\n",
        "\n",
        "        attention_weights = torch.softmax(self.attention(x), dim=1)  # Shape: [batch_size, 1]\n",
        "        x = attention_weights * x  # Apply attention\n",
        "\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for multilabel output\n",
        "        return x\n",
        "\n",
        "# Instantiate model, loss function, optimizer\n",
        "input_dim = X.shape[1]  # Number of features in embeddings\n",
        "output_dim = y_binary.shape[1]  # Number of labels\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HybridMultilabelClassifier(input_dim=input_dim, output_dim=output_dim).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for multilabel\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            predictions.append(outputs.cpu().numpy())\n",
        "            true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    return np.vstack(predictions), np.vstack(true_labels)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred, y_true = evaluate_model(model, val_loader)\n",
        "\n",
        "# Binarize predictions\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, y_pred_binary, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true, y_pred_binary)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLt-_bmOgb7F",
        "outputId": "63f7e407-18d1-4241-aa8d-dc15a855adf9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 6.2968\n",
            "Epoch 2/10, Loss: 6.3053\n",
            "Epoch 3/10, Loss: 6.3092\n",
            "Epoch 4/10, Loss: 6.3073\n",
            "Epoch 5/10, Loss: 6.3112\n",
            "Epoch 6/10, Loss: 6.3112\n",
            "Epoch 7/10, Loss: 6.3092\n",
            "Epoch 8/10, Loss: 6.3092\n",
            "Epoch 9/10, Loss: 6.3092\n",
            "Epoch 10/10, Loss: 6.3092\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   Emotional Distress       0.99      1.00      1.00     10887\n",
            "   Provoking Violence       0.82      1.00      0.90      9012\n",
            "Individual Harassment       0.99      1.00      1.00     10906\n",
            "\n",
            "            micro avg       0.93      1.00      0.97     30805\n",
            "            macro avg       0.93      1.00      0.96     30805\n",
            "         weighted avg       0.94      1.00      0.97     30805\n",
            "          samples avg       0.93      0.99      0.96     30805\n",
            "\n",
            "Overall Accuracy: 0.8201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XU-s0HXYiZzJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}