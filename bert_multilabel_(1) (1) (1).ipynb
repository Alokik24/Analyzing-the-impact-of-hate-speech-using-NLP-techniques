{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36288a39cb6e484d96a9cc8f0c901324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a53d15ba8c646729eb0f4a9fca988c1",
              "IPY_MODEL_e8025612f3b541f9a1a83aadb87bbb65",
              "IPY_MODEL_abf92177c4824004b176b4e0df214aab"
            ],
            "layout": "IPY_MODEL_ac037e8c1b2a47a8a16707accfd324a0"
          }
        },
        "2a53d15ba8c646729eb0f4a9fca988c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_073e9d7590e9448fb0e3d0f047f1c46b",
            "placeholder": "​",
            "style": "IPY_MODEL_384dcfd2f0644a5b9c4ca2ca243e2c2f",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e8025612f3b541f9a1a83aadb87bbb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8578cacb4fdc47f49397ffa5f40d2d42",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ac3da9f4e274d5eb8b2b0298eca129e",
            "value": 48
          }
        },
        "abf92177c4824004b176b4e0df214aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7a9c195451d4eac844dac41ec664852",
            "placeholder": "​",
            "style": "IPY_MODEL_855a164e2c5143c08be43ca41cd247d2",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.46kB/s]"
          }
        },
        "ac037e8c1b2a47a8a16707accfd324a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "073e9d7590e9448fb0e3d0f047f1c46b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384dcfd2f0644a5b9c4ca2ca243e2c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8578cacb4fdc47f49397ffa5f40d2d42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac3da9f4e274d5eb8b2b0298eca129e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7a9c195451d4eac844dac41ec664852": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "855a164e2c5143c08be43ca41cd247d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed13b07acda54adfa38ec167b03b7b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e73179679cc04276b6ee701e7f594a29",
              "IPY_MODEL_2cb051c9ba7a46f795149a2dccc4adeb",
              "IPY_MODEL_1cc9f63e1c694b7c9581cd39e34a6f77"
            ],
            "layout": "IPY_MODEL_6998b572c4204d9eb71cfcac9a183d34"
          }
        },
        "e73179679cc04276b6ee701e7f594a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e3c6c1bde0d4c6c8dcbac51be370b57",
            "placeholder": "​",
            "style": "IPY_MODEL_889a6b198f614af7b9aba92a418b5487",
            "value": "vocab.txt: 100%"
          }
        },
        "2cb051c9ba7a46f795149a2dccc4adeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93fa851fa7a248b9980b35600287f154",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39b20ca6349a4201988f8fc053fbd190",
            "value": 231508
          }
        },
        "1cc9f63e1c694b7c9581cd39e34a6f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_766aa91dc9e848a09a5936f076a8c274",
            "placeholder": "​",
            "style": "IPY_MODEL_70935dbe938c4f1982a1451511767d24",
            "value": " 232k/232k [00:00&lt;00:00, 2.85MB/s]"
          }
        },
        "6998b572c4204d9eb71cfcac9a183d34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e3c6c1bde0d4c6c8dcbac51be370b57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "889a6b198f614af7b9aba92a418b5487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93fa851fa7a248b9980b35600287f154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39b20ca6349a4201988f8fc053fbd190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "766aa91dc9e848a09a5936f076a8c274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70935dbe938c4f1982a1451511767d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a3e1cab96df4f029e5cb74a27be59a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd82c6c143a94bf8916d41c7d44b49af",
              "IPY_MODEL_0d14411f14454aacb81bdfb7183b655a",
              "IPY_MODEL_b92103e44f494e5d96fb01a840daffd0"
            ],
            "layout": "IPY_MODEL_5a1dff69e9d7445e9289c53f99f9e03a"
          }
        },
        "dd82c6c143a94bf8916d41c7d44b49af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d0cd0e49d784b74bc1950357defb2b4",
            "placeholder": "​",
            "style": "IPY_MODEL_27b486978f984f2cb5225f97117c9f56",
            "value": "tokenizer.json: 100%"
          }
        },
        "0d14411f14454aacb81bdfb7183b655a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39215e43a67d40459e7390a2f78a1b7e",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d392e7ec458643ad9d060f4c5503f829",
            "value": 466062
          }
        },
        "b92103e44f494e5d96fb01a840daffd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f339de76ca534675b1a9cb39f6520fb4",
            "placeholder": "​",
            "style": "IPY_MODEL_f5aa484967e4495884f3b5f712aea6ed",
            "value": " 466k/466k [00:00&lt;00:00, 3.47MB/s]"
          }
        },
        "5a1dff69e9d7445e9289c53f99f9e03a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d0cd0e49d784b74bc1950357defb2b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27b486978f984f2cb5225f97117c9f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39215e43a67d40459e7390a2f78a1b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d392e7ec458643ad9d060f4c5503f829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f339de76ca534675b1a9cb39f6520fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5aa484967e4495884f3b5f712aea6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2137c80b877b45b6bb477fec62089af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84cd1add15fe40ebaedf3a7278a91c2c",
              "IPY_MODEL_726c9bf2fc14410cb4b1058b070209f1",
              "IPY_MODEL_63fa7da2439343df87b7c9d08be740c2"
            ],
            "layout": "IPY_MODEL_bd2784937caf4589af0d1574cf561cc4"
          }
        },
        "84cd1add15fe40ebaedf3a7278a91c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b572e9dcd134551a1b9c0d273b973b2",
            "placeholder": "​",
            "style": "IPY_MODEL_b90d5ea055f04b199d8fded274a55c3e",
            "value": "config.json: 100%"
          }
        },
        "726c9bf2fc14410cb4b1058b070209f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67f37ba1436347ccac8f43cf62a270f5",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78a6a895651444b49ea6eb57c2ca8e2d",
            "value": 570
          }
        },
        "63fa7da2439343df87b7c9d08be740c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73ccf9ef802c4242bda8ad1a61994d15",
            "placeholder": "​",
            "style": "IPY_MODEL_08f8f28b6cb54cbcab488e02389d1fe2",
            "value": " 570/570 [00:00&lt;00:00, 27.8kB/s]"
          }
        },
        "bd2784937caf4589af0d1574cf561cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b572e9dcd134551a1b9c0d273b973b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b90d5ea055f04b199d8fded274a55c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67f37ba1436347ccac8f43cf62a270f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78a6a895651444b49ea6eb57c2ca8e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73ccf9ef802c4242bda8ad1a61994d15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f8f28b6cb54cbcab488e02389d1fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99b583ed02b94af0b439bd335e2c46c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3c1e8f2dde84532ae97f03bca782954",
              "IPY_MODEL_15c175c80e4f4af493abfde4dceacdec",
              "IPY_MODEL_e931cf2064914c058c6f9e6e4a6ccb3c"
            ],
            "layout": "IPY_MODEL_50354556a4a044dfa4cb3a5c071cde91"
          }
        },
        "f3c1e8f2dde84532ae97f03bca782954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c487da8e53146c1bb5cb81b78e01181",
            "placeholder": "​",
            "style": "IPY_MODEL_ad68e99b5b1a4e99bd35707d8958b662",
            "value": "model.safetensors: 100%"
          }
        },
        "15c175c80e4f4af493abfde4dceacdec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f187e1eda9054415894090c03909b457",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73d85d6a30fc4e7daf9ebde6f11f6393",
            "value": 440449768
          }
        },
        "e931cf2064914c058c6f9e6e4a6ccb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17b30479767a48d9866a48fe03610e3b",
            "placeholder": "​",
            "style": "IPY_MODEL_00f8c8f6ecb74ce2a099944d09bd93aa",
            "value": " 440M/440M [00:02&lt;00:00, 205MB/s]"
          }
        },
        "50354556a4a044dfa4cb3a5c071cde91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c487da8e53146c1bb5cb81b78e01181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad68e99b5b1a4e99bd35707d8958b662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f187e1eda9054415894090c03909b457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d85d6a30fc4e7daf9ebde6f11f6393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17b30479767a48d9866a48fe03610e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00f8c8f6ecb74ce2a099944d09bd93aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN8Z6aLTQX0_",
        "outputId": "050c69ee-71cd-4d72-83bb-0c90ed4bc8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-4d35d9e97a66>:3: DtypeWarning: Columns (80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,118,119,120,121,122,123,124,125,126,127,128,129,130) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df=pd.read_csv(input_file)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "input_file = 'measuring_hate_speech.csv'\n",
        "df=pd.read_csv(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate 'emotionaldistavg'\n",
        "df['emotionaldistavg'] = df.apply(\n",
        "    lambda row: row[['respect', 'sentiment', 'insult', 'dehumanize', 'humiliate', 'status']].mean()\n",
        "    if row['hate_speech_score'] < 6.5 else None, axis=1)\n",
        "\n",
        "# Calculate 'provokingviolenceavg'\n",
        "df['provokingviolenceavg'] = df[['violence', 'genocide', 'dehumanize', 'attack_defend']].mean(axis=1)\n",
        "\n",
        "# Function to calculate 'provokingviolence'\n",
        "def calculate_provoking_violence(row):\n",
        "    avg = row['provokingviolenceavg']\n",
        "    if 0 <= avg < 1.35:\n",
        "        return 0\n",
        "    elif 1.35 <= avg < 1.5:\n",
        "        return 1\n",
        "    elif 1.5 <= avg < 3.16:\n",
        "        return 2\n",
        "    elif avg >= 3.16:\n",
        "        return 3\n",
        "    return None\n",
        "\n",
        "# Function to calculate 'emotionaldistress'\n",
        "def calculate_emotional_distress(row):\n",
        "    avg = row['emotionaldistavg']\n",
        "    if row['hate_speech_score'] > 6.5:\n",
        "        return 3\n",
        "    elif avg is not None:\n",
        "        if 0 <= avg < 1.35:\n",
        "            return 0\n",
        "        elif 1.35 <= avg < 3.1:\n",
        "            return 1\n",
        "        elif 3.1 <= avg <= 4:\n",
        "            return 2\n",
        "    return None\n",
        "\n",
        "# Function to adjust 'provokingviolence'\n",
        "def adjust_provoking_violence(row):\n",
        "    if row['provokingviolence'] == 2:\n",
        "        if row['violence'] == 0 and row['genocide'] == 0 and (row['dehumanize'] in [2, 3] or row['attack_defend'] in [2, 3]):\n",
        "            return 1\n",
        "        elif row['violence'] == 0 and row['genocide'] == 0 and (row['dehumanize'] in [0, 1] or row['attack_defend'] in [0, 1]):\n",
        "            return 0\n",
        "    return row['provokingviolence']\n",
        "\n",
        "# Apply calculations and adjustments\n",
        "df['emotionaldistress'] = df.apply(calculate_emotional_distress, axis=1)\n",
        "df['provokingviolence'] = df.apply(calculate_provoking_violence, axis=1)\n",
        "df['provokingviolence'] = df.apply(adjust_provoking_violence, axis=1)\n",
        "\n",
        "# Calculate 'individualharassmentavg'\n",
        "df['individualharassmentavg'] = df[['status', 'insult', 'attack_defend', 'dehumanize', 'humiliate']].mean(axis=1)\n",
        "\n",
        "# Function to calculate 'individualharrassment'\n",
        "def calculate_individual_harassment(row):\n",
        "    avg = row['individualharassmentavg']\n",
        "    if 0 <= avg <= 1.1:\n",
        "        return 0\n",
        "    elif 1.1 < avg < 2.8:\n",
        "        return 1\n",
        "    elif 2.8 <= avg < 3.75:\n",
        "        return 2\n",
        "    elif 3.75 <= avg <= 4:\n",
        "        return 3\n",
        "    return None\n",
        "\n",
        "# Apply individual harassment calculation\n",
        "df['individualharrassment'] = df.apply(calculate_individual_harassment, axis=1)\n",
        "\n",
        "# Function to calculate 'hatespeechintensity'\n",
        "def calculate_hatespeech_intensity(row):\n",
        "    if row['hatespeech'] in [0, 1]:\n",
        "        return row['hatespeech']\n",
        "    elif row['hatespeech'] == 2:\n",
        "        if (row[['sentiment', 'respect', 'insult', 'dehumanize', 'humiliate', 'violence', 'attack_defend']].mean()) >= 3.8:\n",
        "            return 3\n",
        "        else:\n",
        "            return 2\n",
        "    return None\n",
        "\n",
        "# Apply the function to generate 'hatespeechintensity'\n",
        "df['hatespeechintensity'] = df.apply(calculate_hatespeech_intensity, axis=1)\n",
        "\n",
        "# Select columns for the first output file\n",
        "columns_to_keep = [\n",
        "    'comment_id', 'annotator_id', 'platform', 'sentiment', 'respect', 'insult', 'humiliate', 'status',\n",
        "    'dehumanize', 'violence', 'genocide', 'attack_defend', 'hatespeech', 'text', 'hate_speech_score',\n",
        "    'emotionaldistavg', 'emotionaldistress', 'provokingviolenceavg', 'provokingviolence',\n",
        "    'individualharassmentavg', 'individualharrassment', 'hatespeechintensity'\n",
        "]\n",
        "\n",
        "# Save the preprocessed dataset into 'updated_dataset_preprocessed.csv'\n",
        "new_df = df[columns_to_keep]\n",
        "new_df.to_csv('updated_dataset_preprocessed.csv', index=False)\n",
        "\n",
        "# Load the new dataframe and select columns for the second output file\n",
        "new_df = pd.read_csv('updated_dataset_preprocessed.csv')\n",
        "columns_to_save = ['comment_id', 'text', 'emotionaldistress', 'provokingviolence', 'individualharrassment', 'hatespeechintensity']\n",
        "\n",
        "# Save the second version into 'dataset_v2.csv'\n",
        "new_df_v2 = new_df[columns_to_save]\n",
        "new_df_v2.to_csv('dataset_v2.csv', index=False)\n",
        "\n",
        "# Print sample data from both datasets\n",
        "print(\"Updated Dataset (Preprocessed):\")\n",
        "print(new_df.head())\n",
        "\n",
        "print(\"\\nDataset V2:\")\n",
        "print(new_df_v2.head())\n",
        "\n",
        "# Print the length of the new dataset\n",
        "print(f\"\\nLength of Dataset V2: {len(new_df_v2)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm-zrpC5Qnbw",
        "outputId": "f17928b5-268e-4bc5-a48f-9407f4c64946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset (Preprocessed):\n",
            "   comment_id  annotator_id  platform  sentiment  respect  insult  humiliate  \\\n",
            "0       47777         10873         3          0        0       0          0   \n",
            "1       39773          2790         2          0        0       0          0   \n",
            "2       47101          3379         3          4        4       4          4   \n",
            "3       43625          7365         3          2        3       2          1   \n",
            "4       12538           488         0          4        4       4          4   \n",
            "\n",
            "   status  dehumanize  violence  ...  hatespeech  \\\n",
            "0       2           0         0  ...           0   \n",
            "1       2           0         0  ...           0   \n",
            "2       4           4         0  ...           2   \n",
            "3       2           0         0  ...           0   \n",
            "4       4           4         4  ...           2   \n",
            "\n",
            "                                                text  hate_speech_score  \\\n",
            "0  Yes indeed. She sort of reminds me of the elde...              -3.90   \n",
            "1  The trans women reading this tweet right now i...              -6.52   \n",
            "2  Question: These 4 broads who criticize America...               0.36   \n",
            "3  It is about time for all illegals to go back t...               0.26   \n",
            "4  For starters bend over the one in pink and kic...               1.54   \n",
            "\n",
            "  emotionaldistavg  emotionaldistress  provokingviolenceavg  \\\n",
            "0         0.333333                  0                  0.00   \n",
            "1         0.333333                  0                  0.50   \n",
            "2         4.000000                  2                  2.00   \n",
            "3         1.666667                  1                  0.75   \n",
            "4         4.000000                  2                  3.00   \n",
            "\n",
            "   provokingviolence  individualharassmentavg  individualharrassment  \\\n",
            "0                  0                      0.4                      0   \n",
            "1                  0                      0.8                      0   \n",
            "2                  2                      4.0                      3   \n",
            "3                  0                      1.6                      1   \n",
            "4                  2                      3.8                      3   \n",
            "\n",
            "   hatespeechintensity  \n",
            "0                    0  \n",
            "1                    0  \n",
            "2                    2  \n",
            "3                    0  \n",
            "4                    3  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "Dataset V2:\n",
            "   comment_id                                               text  \\\n",
            "0       47777  Yes indeed. She sort of reminds me of the elde...   \n",
            "1       39773  The trans women reading this tweet right now i...   \n",
            "2       47101  Question: These 4 broads who criticize America...   \n",
            "3       43625  It is about time for all illegals to go back t...   \n",
            "4       12538  For starters bend over the one in pink and kic...   \n",
            "\n",
            "   emotionaldistress  provokingviolence  individualharrassment  \\\n",
            "0                  0                  0                      0   \n",
            "1                  0                  0                      0   \n",
            "2                  2                  2                      3   \n",
            "3                  1                  0                      1   \n",
            "4                  2                  2                      3   \n",
            "\n",
            "   hatespeechintensity  \n",
            "0                    0  \n",
            "1                    0  \n",
            "2                    2  \n",
            "3                    0  \n",
            "4                    3  \n",
            "\n",
            "Length of Dataset V2: 27345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df1 = pd.read_csv('updated_dataset_preprocessed.csv')\n",
        "\n",
        "new_columns = [\n",
        "    'emotionaldistavg', 'emotionaldistress', 'provokingviolenceavg', 'provokingviolence',\n",
        "    'individualharassmentavg', 'individualharrassment', 'hatespeechintensity'\n",
        "]\n",
        "\n",
        "nan_counts = new_df1[new_columns].isna().sum()\n",
        "\n",
        "print(\"Number of NaN values in each new column:\")\n",
        "print(nan_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liJvWLT8Qpt9",
        "outputId": "c3b46663-9878-4d54-8c0d-519b0087e29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaN values in each new column:\n",
            "emotionaldistavg           0\n",
            "emotionaldistress          0\n",
            "provokingviolenceavg       0\n",
            "provokingviolence          0\n",
            "individualharassmentavg    0\n",
            "individualharrassment      0\n",
            "hatespeechintensity        0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "\n",
        "df = pd.read_csv('updated_dataset_preprocessed.csv')\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "def tokenize_text(text):\n",
        "\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return encoded\n",
        "\n",
        "def embed_text(text):\n",
        "    tokens = tokenize_text(text)\n",
        "    input_ids = tokens['input_ids'].to(device)\n",
        "    attention_mask = tokens['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
        "    return cls_embedding\n",
        "\n",
        "\n",
        "df['embedded_text'] = df['cleaned_text'].apply(embed_text)\n",
        "\n",
        "df['embedded_text'] = df['embedded_text'].apply(lambda x: x.tolist())\n",
        "\n",
        "columns_to_save = [\n",
        "    'comment_id', 'embedded_text', 'emotionaldistress',\n",
        "    'provokingviolence', 'individualharrassment', 'hatespeechintensity'\n",
        "]\n",
        "\n",
        "processed_df = df[columns_to_save]\n",
        "\n",
        "processed_df.to_csv('processed_bert_embedded_dataset.csv', index=False)\n",
        "\n",
        "print(processed_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735,
          "referenced_widgets": [
            "36288a39cb6e484d96a9cc8f0c901324",
            "2a53d15ba8c646729eb0f4a9fca988c1",
            "e8025612f3b541f9a1a83aadb87bbb65",
            "abf92177c4824004b176b4e0df214aab",
            "ac037e8c1b2a47a8a16707accfd324a0",
            "073e9d7590e9448fb0e3d0f047f1c46b",
            "384dcfd2f0644a5b9c4ca2ca243e2c2f",
            "8578cacb4fdc47f49397ffa5f40d2d42",
            "2ac3da9f4e274d5eb8b2b0298eca129e",
            "c7a9c195451d4eac844dac41ec664852",
            "855a164e2c5143c08be43ca41cd247d2",
            "ed13b07acda54adfa38ec167b03b7b31",
            "e73179679cc04276b6ee701e7f594a29",
            "2cb051c9ba7a46f795149a2dccc4adeb",
            "1cc9f63e1c694b7c9581cd39e34a6f77",
            "6998b572c4204d9eb71cfcac9a183d34",
            "0e3c6c1bde0d4c6c8dcbac51be370b57",
            "889a6b198f614af7b9aba92a418b5487",
            "93fa851fa7a248b9980b35600287f154",
            "39b20ca6349a4201988f8fc053fbd190",
            "766aa91dc9e848a09a5936f076a8c274",
            "70935dbe938c4f1982a1451511767d24",
            "5a3e1cab96df4f029e5cb74a27be59a8",
            "dd82c6c143a94bf8916d41c7d44b49af",
            "0d14411f14454aacb81bdfb7183b655a",
            "b92103e44f494e5d96fb01a840daffd0",
            "5a1dff69e9d7445e9289c53f99f9e03a",
            "4d0cd0e49d784b74bc1950357defb2b4",
            "27b486978f984f2cb5225f97117c9f56",
            "39215e43a67d40459e7390a2f78a1b7e",
            "d392e7ec458643ad9d060f4c5503f829",
            "f339de76ca534675b1a9cb39f6520fb4",
            "f5aa484967e4495884f3b5f712aea6ed",
            "2137c80b877b45b6bb477fec62089af1",
            "84cd1add15fe40ebaedf3a7278a91c2c",
            "726c9bf2fc14410cb4b1058b070209f1",
            "63fa7da2439343df87b7c9d08be740c2",
            "bd2784937caf4589af0d1574cf561cc4",
            "1b572e9dcd134551a1b9c0d273b973b2",
            "b90d5ea055f04b199d8fded274a55c3e",
            "67f37ba1436347ccac8f43cf62a270f5",
            "78a6a895651444b49ea6eb57c2ca8e2d",
            "73ccf9ef802c4242bda8ad1a61994d15",
            "08f8f28b6cb54cbcab488e02389d1fe2",
            "99b583ed02b94af0b439bd335e2c46c9",
            "f3c1e8f2dde84532ae97f03bca782954",
            "15c175c80e4f4af493abfde4dceacdec",
            "e931cf2064914c058c6f9e6e4a6ccb3c",
            "50354556a4a044dfa4cb3a5c071cde91",
            "4c487da8e53146c1bb5cb81b78e01181",
            "ad68e99b5b1a4e99bd35707d8958b662",
            "f187e1eda9054415894090c03909b457",
            "73d85d6a30fc4e7daf9ebde6f11f6393",
            "17b30479767a48d9866a48fe03610e3b",
            "00f8c8f6ecb74ce2a099944d09bd93aa"
          ]
        },
        "id": "DoDnPNBFQvv0",
        "outputId": "2d10e395-0cbb-43f5-e130-df1763271bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36288a39cb6e484d96a9cc8f0c901324"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed13b07acda54adfa38ec167b03b7b31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a3e1cab96df4f029e5cb74a27be59a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2137c80b877b45b6bb477fec62089af1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99b583ed02b94af0b439bd335e2c46c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-4183d4a84cbd>:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   comment_id                                      embedded_text  \\\n",
            "0       47777  [-0.08673834055662155, 0.022910749539732933, -...   \n",
            "1       39773  [0.14396293461322784, -0.007813084870576859, 0...   \n",
            "2       47101  [-0.2818196713924408, 0.31229910254478455, -0....   \n",
            "3       43625  [-0.014131826348602772, 0.1035425141453743, -0...   \n",
            "4       12538  [-0.4389498233795166, -0.44695088267326355, -0...   \n",
            "\n",
            "   emotionaldistress  provokingviolence  individualharrassment  \\\n",
            "0                  0                  0                      0   \n",
            "1                  0                  0                      0   \n",
            "2                  2                  2                      3   \n",
            "3                  1                  0                      1   \n",
            "4                  2                  2                      3   \n",
            "\n",
            "   hatespeechintensity  \n",
            "0                    0  \n",
            "1                    0  \n",
            "2                    2  \n",
            "3                    0  \n",
            "4                    3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'processed_bert_embedded_dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df_filtered = df[df['hatespeechintensity'] != 0]\n",
        "df_final = df_filtered.drop('hatespeechintensity', axis=1)\n",
        "df_final.to_csv('final_embedded_dataset.csv', index=False)\n",
        "print(df_final.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loUeo3jvQxwJ",
        "outputId": "571748d7-af76-406f-c44c-54fc5e3eef3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   comment_id                                      embedded_text  \\\n",
            "2       47101  [-0.2818196713924408, 0.31229910254478455, -0....   \n",
            "4       12538  [-0.4389498233795166, -0.44695088267326355, -0...   \n",
            "6       13168  [0.07826884090900421, 0.41834592819213867, 0.2...   \n",
            "7       17034  [0.21192587912082672, 0.45486536622047424, -0....   \n",
            "9        1006  [-0.16936618089675903, 0.056792594492435455, 0...   \n",
            "\n",
            "   emotionaldistress  provokingviolence  individualharrassment  \n",
            "2                  2                  2                      3  \n",
            "4                  2                  2                      3  \n",
            "6                  2                  1                      2  \n",
            "7                  2                  0                      2  \n",
            "9                  2                  2                      3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('final_embedded_dataset.csv')\n",
        "multilabel_df = df[['comment_id', 'embedded_text', 'emotionaldistress', 'provokingviolence', 'individualharrassment']]\n",
        "multilabel_df.to_csv('multilabel.csv', index=False)\n",
        "print(multilabel_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-ZdzHO3SgrW",
        "outputId": "3580a886-d791-470e-e1e6-8d0af8efda88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   comment_id                                      embedded_text  \\\n",
            "0       47101  [-0.2818196713924408, 0.31229910254478455, -0....   \n",
            "1       12538  [-0.4389498233795166, -0.44695088267326355, -0...   \n",
            "2       13168  [0.07826884090900421, 0.41834592819213867, 0.2...   \n",
            "3       17034  [0.21192587912082672, 0.45486536622047424, -0....   \n",
            "4        1006  [-0.16936618089675903, 0.056792594492435455, 0...   \n",
            "\n",
            "   emotionaldistress  provokingviolence  individualharrassment  \n",
            "0                  2                  2                      3  \n",
            "1                  2                  2                      3  \n",
            "2                  2                  1                      2  \n",
            "3                  2                  0                      2  \n",
            "4                  2                  2                      3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count occurrences of 0, 1, 2, and 3 (or 0, 1, 2) in specified columns\n",
        "\n",
        "def count_occurrences(df, columns):\n",
        "    for col in columns:\n",
        "        counts = df[col].value_counts()\n",
        "        print(f\"Counts for column '{col}':\")\n",
        "        print(counts)\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "columns_to_check = ['emotionaldistress', 'provokingviolence', 'individualharrassment']\n",
        "count_occurrences(df, columns_to_check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aZYbk3mShcL",
        "outputId": "50dbbc07-2146-402d-84c3-8e62ffd8088a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for column 'emotionaldistress':\n",
            "emotionaldistress\n",
            "2    5408\n",
            "1    3731\n",
            "0      86\n",
            "Name: count, dtype: int64\n",
            "--------------------\n",
            "Counts for column 'provokingviolence':\n",
            "provokingviolence\n",
            "2    5203\n",
            "0    2368\n",
            "1     980\n",
            "3     674\n",
            "Name: count, dtype: int64\n",
            "--------------------\n",
            "Counts for column 'individualharrassment':\n",
            "individualharrassment\n",
            "2    4736\n",
            "1    2875\n",
            "3    1548\n",
            "0      66\n",
            "Name: count, dtype: int64\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, RepeatVector, TimeDistributed, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "wkBIbRoZSj_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "data = pd.read_csv('multilabel.csv')\n",
        "\n",
        "# Extract features and labels\n",
        "X_embedded = np.array(data['embedded_text'].apply(lambda x: np.fromstring(x[1:-1], sep=',')).tolist())  # Convert embedded_text to array\n",
        "y = data[['emotionaldistress', 'provokingviolence', 'individualharrassment']].values  # Labels\n"
      ],
      "metadata": {
        "id": "76AOYo1tTdTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y to binary format (multi-label)\n",
        "y_binary = (y > 0).astype(int)  # Presence or absence\n"
      ],
      "metadata": {
        "id": "MlK8RZrKUlO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_embedded, y_binary, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "3PcUPUHdUrTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate inverse frequency weights for each class\n",
        "def compute_inverse_freq_weights(y):\n",
        "    class_weights = {}\n",
        "    for i in range(y.shape[1]):  # Loop over each label\n",
        "        label_counts = Counter(y[:, i])  # Count occurrences of 0 and 1 in each label\n",
        "        total = len(y)  # Total number of samples\n",
        "        # Calculate weight for each class as inverse of its frequency\n",
        "        class_weights[i] = {\n",
        "            0: total / (2 * label_counts[0]),  # Weight for class 0 (negative)\n",
        "            1: total / (2 * label_counts[1])   # Weight for class 1 (positive)\n",
        "        }\n",
        "    return class_weights\n",
        "\n",
        "# Compute inverse frequency weights for the training labels\n",
        "class_weights = compute_inverse_freq_weights(y_binary)\n"
      ],
      "metadata": {
        "id": "Y3oaciDIVQ3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the data for CNN input\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n"
      ],
      "metadata": {
        "id": "CohFPMLpVSwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the class weights into a tensor for use in the loss function\n",
        "output_dim = len(class_weights)\n",
        "class_weights_tensor = np.array([[class_weights[i][0], class_weights[i][1]] for i in range(output_dim)], dtype=np.float32)\n",
        "class_weights_tensor = tf.constant(class_weights_tensor)\n",
        "\n",
        "\n",
        "# Define custom weighted binary crossentropy loss function\n",
        "def weighted_binary_crossentropy(y_true, y_pred):\n",
        "    # Calculate the binary crossentropy loss\n",
        "    bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Reshape bce_loss to add a dimension for the output classes\n",
        "    bce_loss = tf.expand_dims(bce_loss, axis=-1) # Add dimension to match weights\n",
        "\n",
        "    # Compute the weights for the true labels (0 or 1)\n",
        "    # using broadcasting for efficient computation\n",
        "    weights = tf.where(tf.equal(y_true, 1),\n",
        "                       class_weights_tensor[..., 1],  # Weight for class 1 (positive)\n",
        "                       class_weights_tensor[..., 0])  # Weight for class 0 (negative)\n",
        "\n",
        "\n",
        "    # Apply the weights to the binary crossentropy loss\n",
        "    weighted_loss = bce_loss * weights\n",
        "\n",
        "    # Return the mean weighted loss across samples and time steps\n",
        "    return tf.reduce_mean(weighted_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "d-mS7Z9aVU_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape y for the decoder to match the output\n",
        "num_time_steps = X_train.shape[1]  # This should match the sequence length in your data\n",
        "y_train_seq = np.repeat(y_train[:, np.newaxis, :], num_time_steps, axis=1)\n",
        "y_val_seq = np.repeat(y_val[:, np.newaxis, :], num_time_steps, axis=1)\n",
        "\n",
        "# Ensure the types are float32\n",
        "y_train_seq = y_train_seq.astype(np.float32)\n",
        "y_val_seq = y_val_seq.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "vIk4qlxNVW5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model creation\n",
        "def create_seq2seq_cnn_model(input_shape, output_dim):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=input_shape)\n",
        "    x = Conv1D(filters=32, kernel_size=2, activation='relu', padding='same')(encoder_inputs)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Conv1D(filters=64, kernel_size=2, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # Flatten the output before Dense\n",
        "    x = Flatten()(x)\n",
        "    encoder_outputs = Dense(128, activation='relu')(x)\n",
        "\n",
        "    # Expand dimensions to match the expected input for LSTM\n",
        "    encoder_outputs = RepeatVector(num_time_steps)(encoder_outputs)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_lstm = LSTM(32, return_sequences=True)(encoder_outputs)\n",
        "    decoder_outputs = TimeDistributed(Dense(output_dim, activation='sigmoid'))(decoder_lstm)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(encoder_inputs, decoder_outputs)\n",
        "    return model\n",
        "\n",
        "# Initialize and compile the model\n",
        "input_shape = (X_embedded.shape[1], 1)  # Input shape\n",
        "output_dim = y_binary.shape[1]  # Output shape\n",
        "model = create_seq2seq_cnn_model(input_shape, output_dim)\n",
        "model.compile(optimizer='adam', loss=weighted_binary_crossentropy, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "JTt-qbp1Vags"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train_seq,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_val, y_val_seq),\n",
        "                    callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL4dE6bVVdcQ",
        "outputId": "3dd21966-fda2-48ce-bf48-8d5fb00b90c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 206ms/step - accuracy: 0.0034 - loss: 0.6319 - val_accuracy: 0.4257 - val_loss: 0.7541\n",
            "Epoch 2/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 113ms/step - accuracy: 0.6133 - loss: 0.6215 - val_accuracy: 0.9968 - val_loss: 0.7348\n",
            "Epoch 3/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.9676 - loss: 0.5278 - val_accuracy: 0.9907 - val_loss: 0.7294\n",
            "Epoch 4/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 103ms/step - accuracy: 0.1615 - loss: 0.4963 - val_accuracy: 0.9968 - val_loss: 0.7224\n",
            "Epoch 5/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - accuracy: 0.8004 - loss: 0.7005 - val_accuracy: 0.9968 - val_loss: 0.8256\n",
            "Epoch 6/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.8271 - loss: 0.6435 - val_accuracy: 0.0352 - val_loss: 0.7110\n",
            "Epoch 7/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 100ms/step - accuracy: 0.1966 - loss: 0.6121 - val_accuracy: 0.9025 - val_loss: 0.7550\n",
            "Epoch 8/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 112ms/step - accuracy: 0.7470 - loss: 0.5539 - val_accuracy: 0.9968 - val_loss: 0.8954\n",
            "Epoch 9/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9756 - loss: 0.5173 - val_accuracy: 0.9554 - val_loss: 0.6918\n",
            "Epoch 10/10\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 101ms/step - accuracy: 0.8871 - loss: 0.4784 - val_accuracy: 0.9946 - val_loss: 0.9136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predictions on the validation set\n",
        "y_pred_probs = model.predict(X_val)\n",
        "\n",
        "# Set a threshold for binary classification based on probabilities\n",
        "threshold = 0.5\n",
        "y_pred = (y_pred_probs[:, -1, :] > threshold).astype(int)  # Get predictions from the last time step\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REVpp6myVfuX",
        "outputId": "f88887ae-ef9d-42ce-c211-bacc756ab204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 388ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classification report for each label\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_pred, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n",
        "# Calculate overall accuracy by comparing the exact match of all labels\n",
        "overall_accuracy = accuracy_score(y_val, y_pred)\n",
        "\n",
        "# Print the overall accuracy\n",
        "print(f\"\\nOverall Accuracy: {overall_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boMknaKBZcwK",
        "outputId": "03241a91-72e5-4499-e2f0-3b66b7eb8814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   Emotional Distress       0.99      1.00      1.00       623\n",
            "   Provoking Violence       0.78      0.54      0.64       460\n",
            "Individual Harassment       0.99      1.00      1.00       623\n",
            "\n",
            "            micro avg       0.95      0.88      0.91      1706\n",
            "            macro avg       0.92      0.85      0.88      1706\n",
            "         weighted avg       0.93      0.88      0.90      1706\n",
            "          samples avg       0.95      0.88      0.90      1706\n",
            "\n",
            "\n",
            "Overall Accuracy: 0.5374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample examples from the validation set\n",
        "def display_examples(X, y_true, y_pred, num_examples=25):\n",
        "    for i in range(num_examples):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(f\"True Labels (y): {y_true[i]}\")\n",
        "        print(f\"Predicted Labels: {y_pred[i]}\\n\")\n",
        "\n",
        "# Display sample examples from the validation set\n",
        "display_examples(X_val, y_val, y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2lhpogwZkJm",
        "outputId": "cbbf8cc1-ad0e-404d-9f7a-d13363c290f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "True Labels (y): [1 0 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 2:\n",
            "True Labels (y): [1 0 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 3:\n",
            "True Labels (y): [1 0 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 4:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 5:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 6:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 7:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 8:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 9:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 10:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 11:\n",
            "True Labels (y): [1 0 0]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 12:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 13:\n",
            "True Labels (y): [0 0 0]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 14:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 15:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 16:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 17:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 18:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 19:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 20:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 21:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 22:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 23:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 24:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 25:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import AdamW\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('multilabel.csv')\n",
        "\n",
        "# Convert the 'embedded_text' to numpy arrays\n",
        "data['embedded_text'] = data['embedded_text'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedded_text'].tolist())  # Convert to numpy array\n",
        "y = data[['emotionaldistress', 'provokingviolence', 'individualharrassment']].values\n",
        "\n",
        "# Convert y to binary format (multi-label)\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a custom Dataset class\n",
        "class MultilabelDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.embeddings[idx], dtype=torch.float32),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = MultilabelDataset(X_train, y_train)\n",
        "val_dataset = MultilabelDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define a transformer-based model for multilabel classification\n",
        "class TransformerMultilabelClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TransformerMultilabelClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)  # Increase input dimension for better learning\n",
        "        self.dropout = nn.Dropout(0.3)  # Dropout layer to prevent overfitting\n",
        "        self.fc2 = nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for multilabel output\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim = X.shape[1]  # Number of features from BERT embeddings\n",
        "output_dim = y_binary.shape[1]  # Number of labels\n",
        "model = TransformerMultilabelClassifier(input_dim, output_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for multilabel\n",
        "optimizer = AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            predictions.append(outputs.cpu().numpy())\n",
        "            true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    return np.vstack(predictions), np.vstack(true_labels)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred, y_true = evaluate_model(model, val_loader)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred, y_true = evaluate_model(model, val_loader)\n",
        "\n",
        "# Binarize predictions\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, y_pred_binary, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true, y_pred_binary)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqG9ehQlZrlU",
        "outputId": "2c6c0c2a-08b9-4e5d-d7cb-db0fe7d28525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.2317\n",
            "Epoch 2/10, Loss: 0.2091\n",
            "Epoch 3/10, Loss: 0.2009\n",
            "Epoch 4/10, Loss: 0.1997\n",
            "Epoch 5/10, Loss: 0.1928\n",
            "Epoch 6/10, Loss: 0.1898\n",
            "Epoch 7/10, Loss: 0.1851\n",
            "Epoch 8/10, Loss: 0.1833\n",
            "Epoch 9/10, Loss: 0.1793\n",
            "Epoch 10/10, Loss: 0.1787\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   Emotional Distress       0.99      1.00      1.00       623\n",
            "   Provoking Violence       0.75      0.95      0.84       460\n",
            "Individual Harassment       0.99      1.00      1.00       623\n",
            "\n",
            "            micro avg       0.91      0.99      0.95      1706\n",
            "            macro avg       0.91      0.98      0.94      1706\n",
            "         weighted avg       0.92      0.99      0.95      1706\n",
            "          samples avg       0.91      0.98      0.94      1706\n",
            "\n",
            "Overall Accuracy: 0.7281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set the device to GPU if available, otherwise fallback to CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move the model to the correct device\n",
        "model.to(device)\n",
        "\n",
        "# Define a function to display examples from the validation set\n",
        "def display_examples(X, y_true, num_examples=25):\n",
        "    # Ensure that y_pred is in binary format\n",
        "    y_pred = (X > 0.4).astype(int)  # Apply threshold to get binary predictions\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        print(f\"Example {i + 1}:\")\n",
        "        print(f\"True Labels (y): {y_true[i]}\")\n",
        "        print(f\"Predicted Labels: {y_pred[i]}\\n\")\n",
        "\n",
        "# Prepare the input tensor and move it to the correct device\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "\n",
        "# Set model to evaluation mode and get predictions\n",
        "model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_val_tensor)  # Get model predictions\n",
        "\n",
        "# Convert predictions to binary format\n",
        "y_pred_binary = (y_pred.cpu().numpy() > 0.5).astype(int)  # Move predictions back to CPU and threshold\n",
        "\n",
        "# Now, display the examples\n",
        "display_examples(y_pred_binary, y_val, num_examples=25)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxr0cUh7lZs0",
        "outputId": "d43eaecb-4aa2-4c06-d834-b10f8f55a2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "True Labels (y): [1 0 1]\n",
            "Predicted Labels: [1 0 1]\n",
            "\n",
            "Example 2:\n",
            "True Labels (y): [1 0 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 3:\n",
            "True Labels (y): [1 0 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 4:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 5:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 6:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 7:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 8:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 9:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 10:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 11:\n",
            "True Labels (y): [1 0 0]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 12:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 13:\n",
            "True Labels (y): [0 0 0]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 14:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 15:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 16:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 17:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 18:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 19:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 20:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 21:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 22:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 23:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 24:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 25:\n",
            "True Labels (y): [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('final_embedded_dataset.csv')\n",
        "\n",
        "# Convert 'embedded_text' to numpy arrays\n",
        "data['embedded_text'] = data['embedded_text'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedded_text'].tolist())\n",
        "y = data[['emotionaldistress', 'provokingviolence', 'individualharrassment']].values\n",
        "\n",
        "# Convert y to binary format (multi-label)\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Adjusted Hybrid CNN-Attention model without LSTM\n",
        "class HybridMultilabelClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(HybridMultilabelClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        # Fully connected layer after flattening\n",
        "        self.fc1 = nn.Linear(128 * (input_dim // 2), 64)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(64, 1)\n",
        "\n",
        "        # Final fully connected layer for classification\n",
        "        self.fc2 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input shape:\", x.shape)\n",
        "        x = x.unsqueeze(1)  # Add channel dimension for Conv1d, Shape: [batch_size, 1, input_dim]\n",
        "        print(\"After unsqueeze for Conv1d:\", x.shape)\n",
        "\n",
        "        x = torch.relu(self.conv1(x))  # Shape: [batch_size, 128, input_dim]\n",
        "        print(\"After Conv1d:\", x.shape)\n",
        "\n",
        "        x = self.pool1(x)  # Shape: [batch_size, 128, input_dim/2]\n",
        "        print(\"After MaxPool1d:\", x.shape)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten for fully connected layer, Shape: [batch_size, 128 * (input_dim // 2)]\n",
        "        print(\"After flattening:\", x.shape)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))  # Shape: [batch_size, 64]\n",
        "        print(\"After first fully connected layer:\", x.shape)\n",
        "\n",
        "        attention_weights = torch.softmax(self.attention(x), dim=1)  # Shape: [batch_size, 1]\n",
        "        x = attention_weights * x  # Apply attention\n",
        "        print(\"After attention mechanism:\", x.shape)\n",
        "\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for multilabel output\n",
        "        print(\"After final fully connected layer:\", x.shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate model, loss function, optimizer\n",
        "input_dim = X.shape[1]  # Number of features in embeddings\n",
        "output_dim = y_binary.shape[1]  # Number of labels\n",
        "# Instantiate and train this modified model\n",
        "model = HybridMultilabelClassifier(input_dim=X.shape[1], output_dim=y_binary.shape[1]).to(device)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for multilabel\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            predictions.append(outputs.cpu().numpy())\n",
        "            true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    return np.vstack(predictions), np.vstack(true_labels)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred, y_true = evaluate_model(model, val_loader)\n",
        "\n",
        "# Binarize predictions\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, y_pred_binary, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true, y_pred_binary)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "X4gtWEu3qu6U",
        "outputId": "83261650-74c0-4cdc-87c2-781be2c65b76"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-36263354052e>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# Evaluation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Calculate optimal thresholds based on precision-recall trade-off\n",
        "def calculate_optimal_thresholds(y_true, y_pred_prob):\n",
        "    thresholds = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        # Calculate precision-recall curve for the label\n",
        "        precision, recall, threshold = precision_recall_curve(y_true[:, i], y_pred_prob[:, i])\n",
        "\n",
        "        # Get threshold that maximizes F1 (balanced precision and recall)\n",
        "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "        best_threshold = threshold[np.argmax(f1_scores)]\n",
        "        thresholds.append(best_threshold)\n",
        "    return thresholds\n",
        "\n",
        "# Calculate predictions with label-specific thresholds\n",
        "def predict_with_custom_thresholds(model, val_loader, thresholds):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].cpu().numpy()\n",
        "\n",
        "            outputs = model(input_ids).cpu().numpy()\n",
        "            predictions.append(outputs)\n",
        "            true_labels.append(labels)\n",
        "\n",
        "    predictions = np.vstack(predictions)\n",
        "    true_labels = np.vstack(true_labels)\n",
        "\n",
        "    # Apply label-specific thresholds\n",
        "    pred_labels = np.zeros_like(predictions)\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        pred_labels[:, i] = (predictions[:, i] > threshold).astype(int)\n",
        "\n",
        "    return pred_labels, true_labels\n",
        "\n",
        "# Get prediction probabilities on validation set\n",
        "y_pred_prob, y_true = evaluate_model(model, val_loader)\n",
        "\n",
        "# Calculate optimal thresholds per label\n",
        "optimal_thresholds = calculate_optimal_thresholds(y_true, y_pred_prob)\n",
        "print(\"Optimal thresholds for each label:\", optimal_thresholds)\n",
        "\n",
        "# Make predictions with these thresholds\n",
        "y_pred_binary, y_true_binary = predict_with_custom_thresholds(model, val_loader, optimal_thresholds)\n",
        "\n",
        "# Display classification report with custom thresholds\n",
        "print(classification_report(y_true_binary, y_pred_binary, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITzUTAqBzPun",
        "outputId": "c7391739-a041-4218-f362-01c89823efa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([5, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([5, 1, 768])\n",
            "After Conv1d: torch.Size([5, 128, 768])\n",
            "After MaxPool1d: torch.Size([5, 128, 384])\n",
            "After flattening: torch.Size([5, 49152])\n",
            "After first fully connected layer: torch.Size([5, 64])\n",
            "After attention mechanism: torch.Size([5, 64])\n",
            "After final fully connected layer: torch.Size([5, 3])\n",
            "Optimal thresholds for each label: [0.47411993, 0.017048985, 0.69163066]\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([5, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([5, 1, 768])\n",
            "After Conv1d: torch.Size([5, 128, 768])\n",
            "After MaxPool1d: torch.Size([5, 128, 384])\n",
            "After flattening: torch.Size([5, 49152])\n",
            "After first fully connected layer: torch.Size([5, 64])\n",
            "After attention mechanism: torch.Size([5, 64])\n",
            "After final fully connected layer: torch.Size([5, 3])\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   Emotional Distress       0.99      1.00      0.99       623\n",
            "   Provoking Violence       0.73      1.00      0.84       460\n",
            "Individual Harassment       0.99      1.00      0.99       623\n",
            "\n",
            "            micro avg       0.90      1.00      0.95      1706\n",
            "            macro avg       0.90      1.00      0.94      1706\n",
            "         weighted avg       0.92      1.00      0.95      1706\n",
            "          samples avg       0.90      0.99      0.94      1706\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated function to make predictions on a few samples with label-specific thresholds\n",
        "def predict_samples_with_thresholds(model, val_loader, thresholds, num_samples=25):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].cpu().numpy()  # True labels for comparison\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(input_ids).cpu().numpy()  # Move predictions to CPU\n",
        "            predictions.append(outputs)\n",
        "            true_labels.append(labels)\n",
        "\n",
        "            if len(predictions) >= num_samples:\n",
        "                break  # Stop after the desired number of samples\n",
        "\n",
        "    # Convert predictions and true labels to numpy arrays\n",
        "    predictions = np.vstack(predictions)\n",
        "    true_labels = np.vstack(true_labels)\n",
        "\n",
        "    # Apply label-specific thresholds to predictions\n",
        "    pred_labels = np.zeros_like(predictions)\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        pred_labels[:, i] = (predictions[:, i] > threshold).astype(int)\n",
        "\n",
        "    # Display true and predicted labels for the samples\n",
        "    for i in range(num_samples):\n",
        "        print(f\"Sample {i + 1}\")\n",
        "        print(f\"True Labels: {true_labels[i]}\")\n",
        "        print(f\"Predicted Labels: {pred_labels[i]}\\n\")\n",
        "\n",
        "# Now use the function to predict and display samples\n",
        "predict_samples_with_thresholds(model, val_loader, optimal_thresholds, num_samples=25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL0yX4EcotR7",
        "outputId": "6ec36da2-c988-47c7-91a2-892609c6d4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Input shape: torch.Size([16, 768])\n",
            "After unsqueeze for Conv1d: torch.Size([16, 1, 768])\n",
            "After Conv1d: torch.Size([16, 128, 768])\n",
            "After MaxPool1d: torch.Size([16, 128, 384])\n",
            "After flattening: torch.Size([16, 49152])\n",
            "After first fully connected layer: torch.Size([16, 64])\n",
            "After attention mechanism: torch.Size([16, 64])\n",
            "After final fully connected layer: torch.Size([16, 3])\n",
            "Sample 1\n",
            "True Labels: [1. 0. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 2\n",
            "True Labels: [1. 0. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 3\n",
            "True Labels: [1. 0. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 4\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 5\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 6\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 7\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 8\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 9\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 10\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 11\n",
            "True Labels: [1. 0. 0.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 12\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 13\n",
            "True Labels: [0. 0. 0.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 14\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 15\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 16\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 17\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 18\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 19\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 20\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 21\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 22\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 23\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 24\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n",
            "Sample 25\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1. 1. 1.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('multilabel.csv')\n",
        "\n",
        "# Convert the 'embedding' column to numpy arrays\n",
        "data['embedded_text'] = data['embedded_text'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedded_text'].tolist())  # Convert to numpy array\n",
        "y = data[['emotionaldistress', 'provokingviolence', 'individualharrassment']].values\n",
        "\n",
        "# Convert y to binary format (multi-label)\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define an XGBoost DMatrix for GPU\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "# Set up XGBoost parameters for GPU usage\n",
        "params = {\n",
        "    'objective': 'binary:logistic',  # Binary classification for each label\n",
        "    'tree_method': 'gpu_hist',       # Use GPU acceleration\n",
        "    'eval_metric': 'logloss',        # Log loss for binary classification\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'learning_rate': 0.01,\n",
        "    'max_depth': 6,\n",
        "    'verbosity': 1\n",
        "}\n",
        "\n",
        "# Train one model per label (multi-label setup)\n",
        "models = []\n",
        "for i in range(y_train.shape[1]):\n",
        "    print(f\"Training model for label {i+1}/{y_train.shape[1]}\")\n",
        "    # Train the model for each label\n",
        "    model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "    models.append(model)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = np.zeros(y_val.shape)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('multilabel.csv')\n",
        "\n",
        "# Convert the 'embedding' column to numpy arrays\n",
        "data['embedded_text'] = data['embedded_text'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedded_text'].tolist())  # Convert to numpy array\n",
        "y = data[['emotionaldistress', 'provokingviolence', 'individualharrassment']].values\n",
        "\n",
        "# Convert y to binary format (multi-label)\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define an XGBoost DMatrix for GPU\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "# Set up XGBoost parameters for GPU usage\n",
        "params = {\n",
        "    'objective': 'binary:logistic',  # Binary classification for each label\n",
        "    'tree_method': 'gpu_hist',       # Use GPU acceleration\n",
        "    'eval_metric': 'logloss',        # Log loss for binary classification\n",
        "    'predictor': 'gpu_predictor',\n",
        "    'learning_rate': 0.01,\n",
        "    'max_depth': 6,\n",
        "    'verbosity': 1\n",
        "}\n",
        "\n",
        "# Train one model per label (multi-label setup)\n",
        "models = []\n",
        "for i in range(y_train.shape[1]):\n",
        "    print(f\"Training model for label {i+1}/{y_train.shape[1]}\")\n",
        "    # Train the model for each label\n",
        "    model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "    models.append(model)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = np.zeros(y_val.shape)\n",
        "for i, model in enumerate(models):\n",
        "    # Predict probabilities for the current label only\n",
        "    y_pred[:, i] = model.predict(dval)[:, i]\n",
        "\n",
        "# Binarize predictions\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print classification report for each label\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_val, y_pred_binary)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Define a function to display examples from the validation set\n",
        "def display_examples(X, y_true, num_examples=10):\n",
        "    for i in range(num_examples):\n",
        "        print(f\"Example {i + 1}:\")\n",
        "        print(f\"True Labels: {y_true[i]}\")\n",
        "        print(f\"Predicted Labels: {X[i]}\\n\")\n",
        "\n",
        "# Display examples from the validation set\n",
        "display_examples(y_pred_binary, y_val, num_examples=10)\n",
        "\n",
        "\n",
        "# Binarize predictions\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print classification report for each label\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_pred_binary, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_val, y_pred_binary)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Define a function to display examples from the validation set\n",
        "def display_examples(X, y_true, num_examples=10):\n",
        "    for i in range(num_examples):\n",
        "        print(f\"Example {i + 1}:\")\n",
        "        print(f\"True Labels: {y_true[i]}\")\n",
        "        print(f\"Predicted Labels: {X[i]}\\n\")\n",
        "\n",
        "# Display examples from the validation set\n",
        "display_examples(y_pred_binary, y_val, num_examples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfc_hnEQloZI",
        "outputId": "60b0d789-cdb4-42c4-f964-9896c83f8dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for label 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for label 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for label 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for label 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for label 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for label 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   Emotional Distress       0.99      1.00      1.00       623\n",
            "   Provoking Violence       0.73      1.00      0.84       460\n",
            "Individual Harassment       0.99      1.00      1.00       623\n",
            "\n",
            "            micro avg       0.90      1.00      0.95      1706\n",
            "            macro avg       0.90      1.00      0.95      1706\n",
            "         weighted avg       0.92      1.00      0.95      1706\n",
            "          samples avg       0.90      0.99      0.94      1706\n",
            "\n",
            "Overall Accuracy: 0.7313\n",
            "Example 1:\n",
            "True Labels: [1 0 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 2:\n",
            "True Labels: [1 0 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 3:\n",
            "True Labels: [1 0 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 4:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 5:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 6:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 7:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 8:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 9:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 10:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   Emotional Distress       0.99      1.00      1.00       623\n",
            "   Provoking Violence       0.73      1.00      0.84       460\n",
            "Individual Harassment       0.99      1.00      1.00       623\n",
            "\n",
            "            micro avg       0.90      1.00      0.95      1706\n",
            "            macro avg       0.90      1.00      0.95      1706\n",
            "         weighted avg       0.92      1.00      0.95      1706\n",
            "          samples avg       0.90      0.99      0.94      1706\n",
            "\n",
            "Overall Accuracy: 0.7313\n",
            "Example 1:\n",
            "True Labels: [1 0 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 2:\n",
            "True Labels: [1 0 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 3:\n",
            "True Labels: [1 0 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 4:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 5:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 6:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 7:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 8:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 9:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 10:\n",
            "True Labels: [1 1 1]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:34:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('final_embedded_dataset.csv')\n",
        "\n",
        "# Convert 'embedded_text' to numpy arrays\n",
        "data['embedded_text'] = data['embedded_text'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedded_text'].tolist())\n",
        "y = data[['emotionaldistress', 'provokingviolence', 'individualharrassment']].values\n",
        "\n",
        "# Convert y to binary format (multi-label)\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Dataset class\n",
        "class MultilabelDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.embeddings[idx], dtype=torch.float32),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = MultilabelDataset(X_train, y_train)\n",
        "val_dataset = MultilabelDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Hybrid Model with CNN, BiLSTM, and Separate Attention Layers\n",
        "class HybridCNNBiLSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(HybridCNNBiLSTMClassifier, self).__init__()\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "\n",
        "        # BiLSTM layer\n",
        "        self.bilstm = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Shared fully connected layer\n",
        "        self.fc_shared = nn.Linear(128 * 2 * (input_dim // 2), 128)  # Adjusted for BiLSTM output\n",
        "\n",
        "        # Separate attention and classification layers for each label\n",
        "        self.attention_emotional = nn.Linear(128, 1)\n",
        "        self.fc_emotional = nn.Linear(128, 1)\n",
        "\n",
        "        self.attention_violence = nn.Linear(128, 1)\n",
        "        self.fc_violence = nn.Linear(128, 1)\n",
        "\n",
        "        self.attention_harassment = nn.Linear(128, 1)\n",
        "        self.fc_harassment = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN\n",
        "        x = x.unsqueeze(1)  # Add channel dimension for Conv1d\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)  # Shape: [batch_size, 256, input_dim / 2]\n",
        "\n",
        "        # BiLSTM\n",
        "        x = x.permute(0, 2, 1)  # Reshape for LSTM: [batch_size, seq_len, features]\n",
        "        lstm_out, _ = self.bilstm(x)  # Shape: [batch_size, input_dim / 2, 256]\n",
        "\n",
        "        # Flatten for fully connected layer\n",
        "        x = lstm_out.contiguous().view(lstm_out.size(0), -1)\n",
        "        shared_features = torch.relu(self.fc_shared(x))  # Shape: [batch_size, 128]\n",
        "\n",
        "        # Emotional Distress Classification with Attention\n",
        "        attn_weights_emotional = torch.softmax(self.attention_emotional(shared_features), dim=1)\n",
        "        emotional_features = attn_weights_emotional * shared_features\n",
        "        emotional_output = torch.sigmoid(self.fc_emotional(emotional_features))\n",
        "\n",
        "        # Provoking Violence Classification with Attention\n",
        "        attn_weights_violence = torch.softmax(self.attention_violence(shared_features), dim=1)\n",
        "        violence_features = attn_weights_violence * shared_features\n",
        "        violence_output = torch.sigmoid(self.fc_violence(violence_features))\n",
        "\n",
        "        # Individual Harassment Classification with Attention\n",
        "        attn_weights_harassment = torch.softmax(self.attention_harassment(shared_features), dim=1)\n",
        "        harassment_features = attn_weights_harassment * shared_features\n",
        "        harassment_output = torch.sigmoid(self.fc_harassment(harassment_features))\n",
        "\n",
        "        # Concatenate outputs for multilabel classification\n",
        "        return torch.cat((emotional_output, violence_output, harassment_output), dim=1)\n",
        "\n",
        "# Instantiate model, loss function, optimizer\n",
        "input_dim = X.shape[1]  # Number of features in embeddings\n",
        "output_dim = y_binary.shape[1]  # Number of labels\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HybridCNNBiLSTMClassifier(input_dim=input_dim, output_dim=output_dim).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].cpu().numpy()\n",
        "\n",
        "            outputs = model(input_ids).cpu().numpy()\n",
        "            predictions.append(outputs)\n",
        "            true_labels.append(labels)\n",
        "\n",
        "    return np.vstack(predictions), np.vstack(true_labels)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred, y_true = evaluate_model(model, val_loader)\n",
        "\n",
        "# Binarize predictions\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, y_pred_binary, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_true, y_pred_binary)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfaClHAwmEZ7",
        "outputId": "edb4f06b-6b2f-4745-8197-02f8cfb09c5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1638\n",
            "Epoch 2/10, Loss: 0.1523\n",
            "Epoch 3/10, Loss: 0.1484\n",
            "Epoch 4/10, Loss: 0.1450\n",
            "Epoch 5/10, Loss: 0.1400\n",
            "Epoch 6/10, Loss: 0.1349\n",
            "Epoch 7/10, Loss: 0.1277\n",
            "Epoch 8/10, Loss: 0.1199\n",
            "Epoch 9/10, Loss: 0.1126\n",
            "Epoch 10/10, Loss: 0.1062\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   Emotional Distress       0.99      1.00      0.99     10887\n",
            "   Provoking Violence       0.85      0.93      0.89      9012\n",
            "Individual Harassment       0.99      1.00      1.00     10906\n",
            "\n",
            "            micro avg       0.95      0.98      0.96     30805\n",
            "            macro avg       0.94      0.97      0.96     30805\n",
            "         weighted avg       0.95      0.98      0.96     30805\n",
            "          samples avg       0.95      0.97      0.95     30805\n",
            "\n",
            "Overall Accuracy: 0.8001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "data = pd.read_csv('final_embedded_dataset.csv')\n",
        "data['embedded_text'] = data['embedded_text'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedded_text'].tolist())  # Features\n",
        "y = data[['emotionaldistress', 'provokingviolence', 'individualharrassment']].values\n",
        "y_binary = (y > 0).astype(int)  # Binarize labels for multi-label classification\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.multi_attn = nn.MultiheadAttention(embed_dim=768, num_heads=12, batch_first=True)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        mv_1, _ = self.multi_attn(q, k, v)\n",
        "        return mv_1\n",
        "\n",
        "class MultiLabelClassificationNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_labels):\n",
        "        super(MultiLabelClassificationNet, self).__init__()\n",
        "        self.multiAttn1 = MultiHeadAttn()\n",
        "        self.multiAttn2 = MultiHeadAttn()\n",
        "        self.multiAttn3 = MultiHeadAttn()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply attention layers\n",
        "        attn_out1 = self.multiAttn1(x, x, x)\n",
        "        attn_out2 = self.multiAttn2(attn_out1, attn_out1, attn_out1)\n",
        "        attn_out3 = self.multiAttn3(attn_out2, attn_out2, attn_out2)\n",
        "\n",
        "        # Use mean pooling to reduce sequence dimension for fully connected layer\n",
        "        x = attn_out3.mean(dim=1)  # (batch_size, embed_dim)\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation for multi-label classification\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = 768  # Example input dimension from embeddings\n",
        "hidden_dim = 128\n",
        "num_labels = y_train.shape[1]\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model, loss, and optimizer\n",
        "model = MultiLabelClassificationNet(input_dim=input_dim, hidden_dim=hidden_dim, num_labels=num_labels).to(device)\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy loss for multi-label\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Move tensors to device\n",
        "X_train_tensor, y_train_tensor = X_train_tensor.to(device), y_train_tensor.to(device)\n",
        "X_val_tensor, y_val_tensor = X_val_tensor.to(device), y_val_tensor.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_val_pred = model(X_val_tensor)\n",
        "    y_val_pred_binary = (y_val_pred > 0.5).int()\n",
        "\n",
        "# Convert predictions and labels to numpy for evaluation\n",
        "y_val_pred_np = y_val_pred_binary.cpu().numpy()\n",
        "y_val_np = y_val_tensor.cpu().numpy()\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val_np, y_val_pred_np, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_val_np, y_val_pred_np)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "\n",
        "# Define a function to display examples from the validation set\n",
        "def display_examples(X, y_true, y_pred, num_examples=10):\n",
        "    for i in range(num_examples):\n",
        "        print(f\"Example {i + 1}:\")\n",
        "        print(f\"True Labels: {y_true[i]}\")\n",
        "        print(f\"Predicted Labels: {y_pred[i]}\\n\")\n",
        "\n",
        "# Display examples from the validation set\n",
        "display_examples(X_val, y_val_np, y_val_pred_np, num_examples=10)\n"
      ],
      "metadata": {
        "id": "iKYamWV5p1ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d38abf-c7ec-4eaf-c716-81a61bff7ef9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6940\n",
            "Epoch [2/10], Loss: 0.6004\n",
            "Epoch [3/10], Loss: 0.1962\n",
            "Epoch [4/10], Loss: 0.3432\n",
            "Epoch [5/10], Loss: 0.2317\n",
            "Epoch [6/10], Loss: 0.3752\n",
            "Epoch [7/10], Loss: 0.3084\n",
            "Epoch [8/10], Loss: 0.3561\n",
            "Epoch [9/10], Loss: 0.2153\n",
            "Epoch [10/10], Loss: 0.2124\n",
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   Emotional Distress       0.99      1.00      1.00     10887\n",
            "   Provoking Violence       0.82      0.99      0.90      9012\n",
            "Individual Harassment       0.99      1.00      1.00     10906\n",
            "\n",
            "            micro avg       0.93      1.00      0.97     30805\n",
            "            macro avg       0.93      1.00      0.96     30805\n",
            "         weighted avg       0.94      1.00      0.97     30805\n",
            "          samples avg       0.94      0.99      0.96     30805\n",
            "\n",
            "Overall Accuracy: 0.8169\n",
            "Example 1:\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 2:\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 3:\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 4:\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 5:\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 6:\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 7:\n",
            "True Labels: [1. 0. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 8:\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 9:\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n",
            "Example 10:\n",
            "True Labels: [1. 1. 1.]\n",
            "Predicted Labels: [1 1 1]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucq22u_pint-",
        "outputId": "902b3e48-6919-4c30-d432-d36969cc0223"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n"
      ],
      "metadata": {
        "id": "j6yx7_K8jwE-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "data = pd.read_csv('final_embedded_dataset.csv')\n",
        "data['embedded_text'] = data['embedded_text'].apply(lambda x: np.fromstring(x[1:-1], sep=','))\n",
        "X = np.array(data['embedded_text'].tolist())  # Features\n",
        "y = data[['emotionaldistress', 'provokingviolence', 'individualharrassment']].values\n",
        "y_binary = (y > 0).astype(int)  # Binarize labels for multi-label classification"
      ],
      "metadata": {
        "id": "ZQLIF__LjzP_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_binary, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RG2m-LSvj_Qj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct graph using k-NN\n",
        "knn = NearestNeighbors(n_neighbors=10, metric='cosine')  # k-NN with cosine distance\n",
        "knn.fit(X_train)\n",
        "distances, indices = knn.kneighbors(X_train)\n"
      ],
      "metadata": {
        "id": "Dun-oSG8kBxn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct edge_index for graph (using k-NN results)\n",
        "edge_index = []\n",
        "for i in range(len(indices)):\n",
        "    for j in indices[i]:\n",
        "        edge_index.append([i, j])\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()"
      ],
      "metadata": {
        "id": "eQtEAGtNo3ET"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "g3o9uSnykpx4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a graph data object for PyTorch Geometric\n",
        "train_data = Data(x=X_train_tensor, edge_index=edge_index, y=y_train_tensor)"
      ],
      "metadata": {
        "id": "hjO8fW-8k2w1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN model\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_labels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return torch.sigmoid(x)  # Multi-label classification"
      ],
      "metadata": {
        "id": "z5omDck7k5Rj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_dim = X_train.shape[1]  # Feature dimension (size of embedded vector)\n",
        "hidden_dim = 128\n",
        "num_labels = y_train.shape[1]\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "yAWr90MCk7oB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, loss, and optimizer\n",
        "model = GCN(input_dim=input_dim, hidden_dim=hidden_dim, num_labels=num_labels)\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy loss for multi-label\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "x4THpG1Jk-iu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(train_data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(output, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUSGwoAxmpbT",
        "outputId": "8d28db69-2ec0-4a3f-c605-dd0769a450cb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5480\n",
            "Epoch [2/10], Loss: 0.3526\n",
            "Epoch [3/10], Loss: 0.2948\n",
            "Epoch [4/10], Loss: 0.2697\n",
            "Epoch [5/10], Loss: 0.2537\n",
            "Epoch [6/10], Loss: 0.2413\n",
            "Epoch [7/10], Loss: 0.2294\n",
            "Epoch [8/10], Loss: 0.2127\n",
            "Epoch [9/10], Loss: 0.2203\n",
            "Epoch [10/10], Loss: 0.2246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct graph for validation data using k-NN\n",
        "knn_val = NearestNeighbors(n_neighbors=10, metric='cosine')\n",
        "knn_val.fit(X_val)\n",
        "distances_val, indices_val = knn_val.kneighbors(X_val)"
      ],
      "metadata": {
        "id": "RdKWj9YOsFYX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct edge_index for validation data (using k-NN results)\n",
        "edge_index_val = []\n",
        "for i in range(len(indices_val)):\n",
        "    for j in indices_val[i]:\n",
        "        edge_index_val.append([i, j])\n",
        "edge_index_val = torch.tensor(edge_index_val, dtype=torch.long).t().contiguous()"
      ],
      "metadata": {
        "id": "k5-RxopNrRJq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create validation graph data object\n",
        "val_data = Data(x=X_val_tensor, edge_index=edge_index_val, y=y_val_tensor)"
      ],
      "metadata": {
        "id": "NItjIbKxpcQ2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(val_data)  # Use the validation data\n",
        "    y_pred_binary_val = (output > 0.5).int()\n",
        "\n",
        "# Ensure correct shape for the validation set\n",
        "y_pred_binary_val_np = y_pred_binary_val.cpu().numpy()\n",
        "y_val_np = y_val_tensor.cpu().numpy()"
      ],
      "metadata": {
        "id": "EOvmkQbUpgdn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val_np, y_pred_binary_val_np, target_names=['Emotional Distress', 'Provoking Violence', 'Individual Harassment']))\n",
        "\n",
        "# Overall accuracy\n",
        "overall_accuracy = accuracy_score(y_val_np, y_pred_binary_val_np)\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYXs9mO_apdW",
        "outputId": "606cc832-55d6-4b0a-ecb7-52322febd78a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "   Emotional Distress       0.99      1.00      1.00     10887\n",
            "   Provoking Violence       0.83      0.97      0.89      9012\n",
            "Individual Harassment       0.99      1.00      1.00     10906\n",
            "\n",
            "            micro avg       0.94      0.99      0.96     30805\n",
            "            macro avg       0.94      0.99      0.96     30805\n",
            "         weighted avg       0.94      0.99      0.97     30805\n",
            "          samples avg       0.94      0.99      0.96     30805\n",
            "\n",
            "Overall Accuracy: 0.8105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Paq381dasPfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}